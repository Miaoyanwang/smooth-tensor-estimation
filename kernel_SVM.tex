\documentclass[11pt]{article}
\usepackage{lscape}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{bm}
\usepackage{gensymb}
\allowdisplaybreaks[4]
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{arydshln}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}


\usepackage{graphics}
\allowdisplaybreaks

\usepackage[utf8x]{inputenc}
\usepackage{bm}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor = blue,
    linkcolor=blue,
    filecolor=magenta,           
    urlcolor=cyan,
}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}
\newtheorem{ass}{Assumption}

\theoremstyle{definition}
\newtheorem{prob}{Problem}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}
\newtheorem{rmk}{Remark}

\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}



\usepackage[labelfont=bf]{caption}

\setcounter{table}{1}
\usepackage{multirow}
\usepackage{tabularx}

\def\fixme#1#2{\textbf{[FIXME (#1): #2]}}

\def\Holder{\text{H\"{o}lder }}

\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}

\begingroup
\makeatletter
\@for\theoremstyle:=definition,remark,plain\do{%
\expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{%
\addtolength\thm@preskip\parskip
}%
}
\endgroup


\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage[parfill]{parskip}
\usepackage{bm}
\onehalfspacing

\newcommand{\maxnorm}[1]{\left\lVert#1\right\rVert_{\infty}}
\newcommand{\Hnorm}[1]{\left\lVert#1\right\rVert_{\tH_\alpha}}
\newcommand{\nullnorm}[1]{\left\lVert#1\right\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%             Math Symbols
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%               Bold Math
\input macros.tex
\def\refer#1{\emph{\color{blue}#1}}
\begin{document}

\begin{center}
{\bf \Large Connection between kernel SMM and SVM}\\
Miaoyan Wang, May 18, 2020\\
\end{center}

\vspace{.5cm}
\begin{comment}
\section{Model}
Key: Cartesian product of piecewise constant function representations. (has full generality...)

Let $\tY=\entry{y_{i_1,\ldots,i_K}}\in\{0,1\}^{d_1 \times \cdots \times d_K}$ be an order-$K$, $(d_1,\ldots,d_K)$-dimensional binary tensor. Let $\mxi^{(k)}=(\xi^{(k)}_1,\ldots,\xi^{(k)}_d)\in[0,1]^d$ be random vectors following (unknown) distributions $\mathbb{P}^{(k)}$ for all $k\in[K]$, and $\mxi^{(k)}$ and $\mxi^{(k')}$ are mutually independent for $k\neq k' \in[K]$. Assume that, conditional on $\{\mxi^{(k)}\}$, the entries of $\tY$ are independent sub-Gaussian distributed:
\[
\mathbb{E}\left(y_{i_1,\ldots,i_K}|\mxi \right)=f\left(\xi_{1,i_1},\ldots,\xi_{K,i_K}\right),\quad \text{for all } (i_1,\ldots,i_K)\in[d]\times \cdots \times [d],
\]
where $f\colon [0,1]^K\mapsto [0,1]$ is an unknown multivariate function belonging to a function class $f\in \tF_\alpha(L)$. Specifically, the function class is defined as
 \[ 
\tF_\alpha(L)=\{f\colon \text{Im}(f)\in[0,1]\ \text{and}\ \Hnorm{f}\leq L\},
\]
where $\alpha\in(0,1]$ is the smoothness parameter and $L>0$ is the H\"{o}lder norm bound for the functions in the class. 

Recall that the function H\"{o}lder norm $\Hnorm{f}$ is defined as
\[
\Hnorm{f}\stackrel{\text{def}}{=}\max_{|\omega|\leq \lfloor \alpha \rfloor}\sup_{\mx \in \tD} |\nabla_{\omega} f(\mx)|+\max_{|\omega| =\lfloor \alpha \rfloor}\sup_{\mx\neq \mx' \in \tD} {\nabla_{\omega} |f(\mx)-\nabla_{\omega} f(\mx')|\over \onenorm{\mx-\mx'}^{\alpha-\lfloor \alpha \rfloor}},
\]
where we have used the short-hand notion
\[
 \nabla_{\omega} f(\mx) ={\partial^{i_1+\cdots+i_K}\over \partial x^{i_1}_1\cdots \partial x^{i_K}_K} f(x_1,\ldots,x_k),
\]
for multi-indices $\omega=(i_1,\ldots,i_K)$ with $|\omega|=i_1+\cdots+i_K$, and $\mx=(x_1,\ldots,x_K)$ in the function domain. 


\section{Estimation}
Define the objective function
\begin{align}
F(\tC, \{\mM_k\}) &= \FnormSize{}{ \tY-\tC\times_1\mM_1\times \cdots \times_K \mM_K}^2.
\end{align}
Denote $\Theta =  \tC\times_1\mM_1\times \cdots \times_K \mM_K$ and $\mr=(r_1,\ldots,r_K)$. Then the feasible domain is
\begin{align}
\tP(r)=& \left\{  \Theta\in\mathbb{R}^{d_1\times \cdots \times d_K} \colon \Theta =  \tC\times_1\mM_1\times \cdots \times_K \mM_K, \text{ where $\tC\in\mathbb{R}^{r_1\times \cdots \times r_K}$ and }\right.\\
& \left.\hspace{1.2in}\mM_k\in\{0,1\}^{d_k\times r_k} \text{ are membership matrices for all $k\in[K]$}\right\}.
\end{align}
%\begin{align}
%\tP(\mr,M)=\{(\tC,\mM_1,\ldots,\mM_K) \colon& \tC\in\mathbb{R}^{r_1\times \cdots \times r_K},\  \maxnorm{\tC}\leq M,\ \text{and } \mM_k\in\{0,1\}^{d_k\times r_k} \text{ are membership}\\
%&\text{ matrices for all $k\in[K]$}\}.
%\end{align}

We propose constrained least-square estimator
\[
\hat \Theta (\mr, M)=\argmin_{\Theta \in \tP(\mr), \maxnorm{\Theta}\leq M} F(\Theta\big|\tY).
\]
The function estimator $\hat f\colon (0,1]^K \mapsto [0,1]$ is defined as follows:
\[
\hat f(x_1,\ldots,x_K)\stackrel{\text{def}}{=}\hat \Theta (\lceil d_1 x_1 \rceil,\ldots,\lceil d_K x_K\rceil),\quad \text{for all }(x_1,\ldots,x_K)\in(0,1]^K. 
\]


We propose an adaptive smooth (?) estimation, 
\begin{align}
\hat \Theta&=\argmin_{\Theta \in\tP(\mr*)} F(\Theta), \quad \text{with}\quad \mr^*=(r^*_1,\ldots,r^*_K),\\
\text{and}\quad &r^*_k=\lceil d_k^{1/( \alpha\wedge 1+1)}\rceil \ \text{for all $k\in[K]$}.
\end{align}

\begin{thm} Consider a function class $\tF_\alpha(R)$ with $\alpha>0$ and $M>0$. We have
\[
\sup_{f \in \tF_\alpha(R)}\sup_{\mxi^{(k)} \sim \mathbb{P}^{(k)}, k\in[K]}{1\over d^K} \mathbb{E}\left(\FnormSize{}{\hat \Theta-f(\xi^{(1)}_{i_1},\ldots,\xi^{(K)}_{i_K})}^2\right) \leq 
C\left(d^{-K\alpha /( \alpha+1)}+{\log d \over d^{K-1}}\right),
\]
where the constant $C>0$ depends only on $L$, and the expectation is taken jointly over $\tY$, $\{\mxi^{(k)}\}$ for all $k\in[K]$.
\end{thm}

{\color{red}Phase transition at $\alpha=1$ only for $K\geq 3$??}
\subsection{Non-parametric tensor model}

Special cases: low-rank model, additive-multiplicative model (in the literature of non-parametric...)

Let $\tY=\entry{Y_\omega}$ be a binary tensor, where $\omega= (i_1,\ldots,i_K)$ is a $K$-tuple index. We propose the following conditionally-independent tensor model:
\begin{align}
Y_\omega | \mxi_\omega &\sim \text{Bernoulli}(\theta_\omega),\\
\theta_\omega&=f(\xi^{(1)}_{i_1},\ldots,\xi^{(K)}_{i_K}),\ \text{for all }\omega=(i_1,\ldots,i_K)\in[d_1]\times \cdots \times [d_K], 
\end{align}
where $f\colon [0,1]^K\mapsto [0,1]$ is a multivariate function of interest. We use $\mxi_\omega\equiv (\xi^{(1)}_{i_1},\ldots,\xi^{(K)}_{i_K})$ to denote the latent design variables at position $\omega= (i_1,\ldots,i_K)$. Furthermore, we assume that the collection of latent variables at the $k$-th coordinate, $(\xi^{(k)}_1,\ldots,\xi^{(k)}_{d_K})$, follow a $d_k$-dimensional distribution $\mathbb{P}^{(k)}$, and the distributions $\mathbb{P}^{(k)}$ and $\mathbb{P}^{(k')}$ are mutually independent for $k\neq k' \in[K]$.

\begin{align}
\mxi\colon [d_1]\times \cdots \times [d_K]&\mapsto [0,1]^K\\
\omega\equiv (i_1,\ldots,i_K)&\mapsto \mxi_\omega\equiv (\xi^{(1)}_{i_1},\ldots,\xi^{(K)}_{i_K})\sim \mathbb{P}^{(1)}\times \cdots \times \mathbb{P}^{(K)}.
\end{align}
\begin{align}
f\colon[0,1]^K&\mapsto[0,1]\\
\mxi&\mapsto f(\mxi).
\end{align}
\begin{lem}[Connection between tensor block model and non-parametric tensor model] Let $f\in\tF_\alpha(L)$ be a target function and $\mxi\sim \mathbb{P}_{\mxi}$ be realized latent variables. Let $f(\mxi_\omega)\in\mathbb{R}$ denote the tensor entry indexed by $\omega\in[d_1]\times \cdots \times [d_K]$, and let $f(\mxi)=\entry{f(\mxi_\omega)}\in\mathbb{R}^{d_1\times \cdots \times d_K}$ denote the non-parametric tensor parameter. Then for each $\mr\in[d_1]\times \cdots \times [d_K]$, there exists a parameter $ \Theta=\entry{ \theta_\omega} \in\tP(\mr)$ such that
%\[
%{1\over \prod_k d_k}\sum_{\omega}\left(f(\mxi_\omega)- \theta_\omega\right)^2\leq L^2\left(K\over r_{\omegan}\right)^{\alpha\wedge 1}.
%\]
\[
\text{Loss}(f(\mxi),\ \Theta)\leq L^2\left(\sum_k {1\over r_k}\right)^{\alpha\wedge 1}.
\]
\end{lem}
\begin{rmk}
The (deterministic) bound holds uniformly over $\mathbb{P}_{\mxi}$ and $\tF_\alpha(L)$.
\end{rmk}

\begin{proof}
The proof is constructive. We partition the interval $[0,1]$ into $r_k$ equal-sized intervals at each of the $K$ modes. Denote the grid of intervals $\tI_{s_1,\ldots,s_K}=\left({s_1-1\over r_1}, {s_1\over r_1}\right ]\times \cdots \times \left({s_K-1\over r_K}, {s_K\over r_K}\right ]$ for all $(s_1,\ldots,s_K)\in[r_1]\times \cdots \times [r_K]$. We use the notation $\mxi_{\omega'}\sim \mxi_{\omega}$ if and only if $\mxi_{\omega'}$ and $\mxi_{\omega}$ belong to the same grid. 

The parameter $\Theta=\entry{ \theta_\omega}$ is constructed as follows. For each $\omega\in[d_1]\times \cdots \times [d_K]$, we set $\theta_\omega$ to be the average of $f(\mxi_{\omega'})$ over the index set for which $\mxi_{\omega'}$ in the same grid as $\mxi_{\omega}$. Specifically, define
\[
\theta_\omega={1\over c_\omega}  \sum_{\omega'} f(\mxi_{\omega'})\mathds{1}\{\mxi_{\omega'}\sim \mxi_\omega\},
\] 
where $c_\omega=\sum_{\omega'}\mathds{1}\{\mxi_{\omega'}\sim \mxi_\omega \}$ is the number of tensor entries in the index set $\{\omega'\colon \mxi_{\omega'}\sim \mxi_{\omega}\}$. 
The construction implies that $\Theta=\entry{\theta_\omega}$ takes constant value in the index set $\{\omega\colon \mxi_\omega\in \tI_{s_1,\ldots,s_K}\}$ for any given $(s_1,\ldots,s_K)\in[r_1]\times \cdots \times [r_K]$. Therefore, $\Theta$ is a block tensor with at most $r$ blocks on each of the $K$ modes; that is, $\Theta\in\tP(\mr)$. 

We will show that the defined $\Theta$ is close to $f(\mxi_\omega)$ in the square distance. Specifically, 
\begin{align}
\left|f(\mxi_\omega)-\theta_\omega\right|&=\left|f(\mxi_\omega)-{1\over c_\omega}  \sum_{\omega'} f(\mxi_{\omega'})\mathds{1}\{\mxi_{\omega'}\sim \mxi_\omega\}\right|\\
 &\leq {1\over c_\omega}\sum_{\{\omega'\colon \mxi_{\omega'}\sim \mxi_\omega\}} \left|f(\mxi_\omega)-f(\mxi_{\omega'})\right|\\
 &\leq {1\over c_\omega} \sum_{\{\omega'\colon \mxi_{\omega'}\sim \mxi_\omega\}} L \nullnorm{\mxi_\omega-\mxi_{\omega'}}^{\alpha\wedge 1}\\
 &\leq {1\over c_\omega} \sum_{\{\omega'\colon \mxi_{\omega'}\sim \mxi_\omega\}} L \left(K\over r\right)^{\alpha\wedge 1}\\
 &\leq LK r^{-(\alpha\wedge 1)},
\end{align}
where the third line comes form the \Holder condition for $f$, the fourth line comes from the fact that $\nullnorm{\mxi_\omega-\mxi_{\omega'}}\leq {K\over r}$ for $\mxi_\omega\sim \mxi_{\omega'}$. Summing over all $\omega \in[d_1]\times \cdots\times [d_K]$ gives the conclusion
\[
{1\over \prod_k d_k }\sum_{\omega}\left(f(\mxi_\omega)- \theta_\omega\right)^2\leq  L^2K^2r^{-2(\alpha\wedge 1)}.
\]
\end{proof}

\begin{thm}[MSE for tensor block model] Let $\tY\in\{0,1\}^{d_1\times \cdots \times d_K}$ be a binary tensor generated from tensor block model, i.e.
\[
\tY|\Theta\sim \text{Bernoulli}(\Theta),\quad \text{where}\ \Theta\in\tP(\mr).
\]
Then for constant $C'>0$, there exists a constant $C>0$ such that
\[
{1\over \prod_k d_k}\FnormSize{}{\Theta-\hat \Theta}^2\leq C\left( \prod_k {r_k\over d_k} +{\sum_k d_k \log r_k \over \prod_k d_k} \right),
\]
with probability at least $1-\exp(-C'\sum_k d_k \log r_k)$, uniformly over $\Theta\in\tP(\mr)$. 
\end{thm}
Suppose $r\asymp d^{\delta}$ for some $\delta\in[0,1]$.
\[
\text{MSE}(\Theta, \hat \Theta)\asymp
\begin{cases}
d^{-K}, & \delta=0\text{ and } r= 1,\\
d^{-(K-1)},&  \delta=0 \text{ and } r \geq 2,\\
d^{-(K-1)}\log d,&\delta\in\left(0,{1\over K}\right],\\
d^{-K(1-\delta)},&\delta\in\left({1\over K},1\right].
\end{cases}
\]

\begin{thm}
Assume $r_k=O(r)$ and $d_k =O(d)$. Then 
\[
\text{MSE}(f(\mxi), \hat \Theta) \leq r^K+ d \log r + L^2K^2r^{-2(\alpha\wedge 1)}.
\]
\end{thm}
\begin{proof}
Taking $r=d^{\delta}$ where $\delta={K\over  2(\alpha \wedge 1)+K}$ gives 
\[
\text{MSE}(f(\mxi), \hat \Theta)\leq 
\begin{cases}
d^{-2\alpha/(2\alpha+1)}+o(1),&K=1\\
{d^{-2\alpha /(\alpha +1)}}+{\log d \over d},& K=2\\
{d^{-2\alpha K /(2\alpha+K)}+d^{-2K/(K+2)}},&K\geq 3,
\end{cases}
\]
Only non-parametric rate appear for $K\geq 3$.
\end{proof}

\begin{rmk}
The distribution of $\mathbb{P}_{\mxi}$ is required to cover $[0,1]$. --  in order to prove the lower bound..
\end{rmk}

We restrict to i.i.d.\ uniform distribution over $[0,1]$. (Erdos Roni)
\begin{thm} There exists a constant $C>0$ only depending on $L$, $\alpha$, such that 
\[
\inf_{\hat \Theta}\sup_{f\in\tF_\alpha(L)}\sup_{\mathbb{P}_{\mxi}\in \tP}\mathbb{E}\{MSE(\hat \Theta, f(\mxi))\}
\begin{cases}
{d^{-2\alpha K /(2\alpha+K)}},& 0< \alpha<1,\\
d^{-1}\log d, &\alpha\geq 1 \text{ and }K=2,\\
d^{-2K/(K+2)}, &\alpha\geq 1 \text{ and }K\geq 3.
\end{cases}
\]
\end{thm}

\section{Assumptions on function families}
We consider two families of functions.

\begin{defn}[$\tF(R)$, piecewise constant functions with at most $r$ marginal pieces] A function $f\colon [0,1]^K\mapsto [0,1]$ is called a multivariate $R$-step function, if there exists a set of mappings $\left\{\phi_k\colon [0,1] \mapsto [R]\right\}_{k\in[K]}$ and an order-$K$ tensor $\tC\in\mathbb{R}^{R\times \cdots \times R}$ such that
\[
f(x_1,\ldots,x_K)=\tC(\phi_1(x_1),\ldots,\phi_K(x_K)),\quad \text{for all } (x_1,\ldots,x_K)\in[0,1]^K.
\]
Denote $\mx=(x_1,\ldots,x_K)^T$ and $\phi(\mx)=(\phi_1(x_1),\ldots,\phi_K(x_K))^T$. Then $f$ can be equivalently written as
\[
f(\mx)=\sum_{\mr\in[R]^K}\tC(\mr)\mathds{1}\left\{\phi(\mx)=\mr\right\},\ \text{for all }\mx\in[0,1]^K.
\]
\end{defn}
\begin{rmk}
The number of constant pieces need not to be equal along each of the $K$ modes. We use $R$ to denote the upper bound for the number of constant pieces over the $K$ modes. 
\end{rmk}

\begin{defn}[$\tF(\alpha,L)$, \Holder smooth functions] Let $\alpha\in(0,1]$ and $L>0$. A function $f\colon [0,1]^K\mapsto [0,1]$ is called an $\alpha$-\Holder smooth function if 
\[
|f(\mx)-f(\mx')|\leq L\onenorm{\mx-\mx'}^\alpha,\quad \text{for all } \mx,\mx'\in[0,1]^K.
\]
The constant $\alpha\in(0,1]$ is called the \Holder smoothness parameter and $L>0$ is the \Holder constant. 
\end{defn}

\begin{defn}[Measure-preserving bijection] Let $\tau\colon \tX\mapsto\tX$ be a bijection and $U$ be a random variable taking values in $\tX$. Then, $\tau$ is called a measure-preserving bijection with respect to $U$ if for all $A\subset\tX$.
\[
\mathbb{P}(U\in A) = \mathbb{P}(\tau(U)\in A).
\] 
In particular, $\tau(U)$ and $U$ are identically distributed. 
\end{defn}
Equivalent class ??
\[
\{f'\colon f'(\mxi')\sim f(\mxi) \text{ whenever }\mxi' \sim \mxi\}
\]

\begin{defn}[Weakly isomorphism] Two functions $f,f'\colon[0,1]^K\mapsto[0,1]$ are called weakly isomorphic if there exists a set of measure-preserving bijections $\{ \tau_k\colon[0,1]\mapsto [0,1]\}_{k\in[K]}$ such that
\[
f(\xi_{1,_1},\ldots,\xi_{K,i_K})\stackrel{a.s.}{=}f'(\tau_1(\xi_{1,i_1}),\ldots,\tau_K(\xi_{K,i_K})),\quad \text{for all }(i_1,\ldots,i_K)\in[d_1]\times\cdots \times [d_K],
\]
where $\{\xi_{k,i_k}\}$ are i.i.d.\ $U[0,1]$ for all $i_k\in[d_k]$ and all $k\in[K]$. We write $f\sim f'$ to denote weakly isomorphism. The weakly isomorphism relationship defines a quotient space in $\tF(R)$ or $\tF(\alpha, L)$. %We use $[f]$ to denote the equivalent class of functions that contains function $f$. 
\end{defn}

\begin{defn}[Index permutation] Two tensors $\Theta, \Theta' \in\mathbb{R}^{d_1\times \cdots \times d_K}$ are called equivalent if there exist a set of index permutations $\{\sigma_k: [d_k]\mapsto[d_k]\}_{k\in[K]}$, such that
\[
\Theta(i_1,\ldots,i_K)=\Theta'(\sigma_1(i_1),\ldots,\sigma_K(i_K)),\quad \text{for all } (i_1,\ldots,i_K)\in[d_1]\times \cdots \times [d_K].
\]
\end{defn}

Two forms of loss are considered.
\begin{defn}[Integrated loss] Let $f\colon[0,1]^K\mapsto[0,1]$ be the function of interest. We define the integrated loss 
\[
\text{Loss}(f, \hat f)\stackrel{\text{def}}{=}\inf_{f_{\text{iso}}\sim f}\int_{\mx\in[0,1]^K}\left|f'(\mx)-f_{\text{iso}}(\mx)\right|^2d\mx,
\]
where $f_{\text{iso}}\sim f$ denotes all (?need to be in the specified function space?) functions that are isomorphic with $f$.
\end{defn}

\begin{defn}[Discrete loss] %Let $f\colon[0,1]^K\mapsto[0,1]$ be the function of interest. Let $\mxi_\omega=(\xi_{1,i_1},\ldots,\xi_{K,i_K})^T$ denote the realized latent variable at position $\omega=(i_1,\ldots,i_K)\in[d_1]\times \cdots \times [d_K]$. Define $\theta_\omega=f(\mxi_\omega)$ (respectively, $\hat \theta_\omega=\hat f(\mxi_\omega)$) and the Bernoulli probability tensor $\Theta=\entry{\theta_\omega}$ (respectively, $\hat \Theta=\entry{\hat \theta_\omega}$). We define the mean squared error (MSE) as
Let $\Theta, \hat \Theta\in\mathbb{R}^{d_1\times \cdots \times d_K}$ be two tensors. We define the discrete loss as
\[
\text{Loss}(\Theta, \hat \Theta )\stackrel{\text{def}}{=}  {1\over \prod_k d_K}\FnormSize{}{\Theta-\hat \Theta}^2.
\]
\end{defn}

\begin{rmk}
We use the notion $\text{Loss}(\cdot, \cdot)$ to denote either the discrete loss (for tensors) or the integrated loss (for functions). The meaning should be clear given the contexts. 
\end{rmk}

\begin{defn}[Operations between $f$ and $\Theta$] Let $f\colon [0,1]^K \mapsto[0,1]$ be a $K$-variate function. Then the $f$-induced probability tensor $\Theta$ is defined as
\[
\Theta(i_1,\ldots,i_K)=f(\xi_{1,i_1},\ldots,\xi_{K,i_K}),\quad \text{for all }(i_1,\ldots,i_K)\in[d_1]\times \cdots \times [d_K],
\]
where $\{\xi_{k,i_k}\}$ are i.i.d.\ \text{Uniform}[0,1] for all $i_k\in[d_k]$ and all $k\in[K]$. 

Conversely, let $\Theta\in\mathbb{R}^{d_1\times \cdots \times d_K}$ be an order-$K$ tensor. Then the $\Theta$-induced function is defined as
\begin{align}
f(x_1,\ldots,x_K)&=\Theta(\lceil d_1x_1\rceil,\ldots,\lceil d_Kx_K\rceil)\\
&=\sum_{(i_1,\ldots,i_K)} \Theta(i_1,\ldots,i_K)\mathds{1}\{(i_k-1) < x_kd_k \leq i_k,\text{ for all }k\in[K]\},
\end{align}
for all $(x_1,\ldots,x_K)\in[0,1]^K$.

\end{defn}
\begin{rmk}
The two operations are not inverse. $f\Rightarrow \Theta(f,\mxi)\Rightarrow f'\stackrel{\text{def}}{=}f(\Theta(f,\mxi))$, but $f \neq f'$ (not even isomorphic). Similarly, $\Theta \Rightarrow f(\Theta,\mxi)\Rightarrow \Theta'\stackrel{\text{ref}}{=}\Theta(f(\Theta,\mxi))$, but $\Theta'\neq \Theta$. The inverse relationship holds only if $\xi_{i_k,k}={i_k\over d_k}$ for all $i_k\in[d_k]$ and all $k\in[K]$.
\end{rmk}

\begin{prop}[Connection between $f$ and $\Theta$] The following properties hold:
\begin{enumerate}[]
\item[] 1. [From functions to tensors] Let $\Theta$ denote the induced tensor from $f$. Then, $f\in \tF(R) \Rightarrow \Theta\in \tP(R,1)$. Furthermore, $\Theta'\sim \Theta \Rightarrow $ there exists $f'$ such that $\Theta'$ is the induced tensor from $f'$.
\item[] 2. [From tensors to functions] Let $f$ denote the induced function from $\Theta$. Then $\Theta\in \tP(R,1) \Rightarrow f\in \tF(R)$.
\item[] 3. [From tensor pairs to function pairs] Let $f,\ f'$ denote the induced functions from $\Theta$ and $\Theta'$, respectively. Then, $f\sim f' \Leftrightarrow \Theta\sim \Theta'$. Furthermore, 
\[
\text{Loss}(f,\ f')\leq \text{Loss}(\Theta,\ \Theta').
\]
\end{enumerate}
\end{prop}

 \begin{prob}[Non-parametric estimation with unobserved designs] Let $f\colon [0,1]^K\mapsto [-1,1]$ be a target function of interest. For each $k\in[K]$, we draw a random i.i.d.\ sample of $d$ points $\{ x^{(k)}_i\}_{i\in[d]}$ uniformly from $[0,1]$. Write $\omega=(i_1,\ldots,i_K)\in[d]^K$, $\mx_\omega=(x^{(1)}_{i_1},\ldots,x^{(K)}_{i_K})^T\in\mathbb{R}^K$, and $f_\omega=f(\mx_\omega)\in \mathbb{R}$. The goal is to estimation $f$ given the set $\{\left(\omega, f_\omega\right)\}$ but without knowing $\{\mx_\omega\}$! 
\end{prob}

We estimate $f$ by using the null design; that is, we partition $[0,1]^K$ into $d^K$ equal-sized grids. Specifically, we assume $\check x^{(k)}_i={i\over d}$ for all $i\in[d]$ and $k\in[K]$, and write $\check \mx_\omega=(\check x^{(1)}_{i_1},\ldots,\check x^{(K)}_{i_K})^T\in\mathbb{R}^K$. Then the estimation is based on $\{(\check \mx_\omega,f_\omega)\}$.

Q1: existing results for known $\{\mx_\omega\}?$ Q2: quantify the difference between $\{\mx_\omega\}$ vs.\ $\{\check \mx_\omega\}$.

The following results give the estimation error when $f$ belongs to $\tF(R,1)$ or $\tF(\alpha,L)$.

\begin{prop}[Agnostic error] Consider the chain $f\Rightarrow \Theta(f,\mxi)\Rightarrow \hat f\stackrel{\text{def}}{=}f(\Theta(f,\mxi))$. 
\begin{itemize}
\item If $f\in \tF(R,1)$, then
\[
\mathbb{E}\left[\text{Loss}(f,\ \hat f)\right]\leq {CKR\over \sqrt{d}},
\]
where the expectation is over $\mxi\sim \mathbb{P}_{\mxi}$.
\item If $f\in \tF(\alpha,L)$, then
\[
\mathbb{E}\left[\text{Loss}(f,\ \hat f)\right]\leq {CK^2L^2\over d^{\alpha}},
\]
where the expectation is over $\mxi\sim \mathbb{P}_{\mxi}$.
\end{itemize}
\end{prop}

\begin{rmk}
Does it matter how to define $\hat f$? say kernel, or smooth version?
\end{rmk}

\begin{proof} 
Note that 
\[
\Theta(i_1,\ldots,i_K)=f(\xi^{(1)}_{i_1},\ldots,\xi^{(K)}_{i_K}),\quad \text{for all }(i_1,\ldots,i_K)\in[d]^K.
\]
For each $k\in[K]$, we sort $\{\xi^{(k)}_i\}_{i\in[d]}$ in an increasing order and write $0\leq \xi^{(k)}_{(1)}\leq \cdots \leq \xi^{(k)}_{(d)}\leq 1$. Define
\[
\Theta^*(i_1,\ldots,i_K)=f(\xi^{(1)}_{(i_1)},\ldots,\xi^{(K)}_{(i_K)}),\quad \text{for all }(i_1,\ldots,i_K)\in[d]^K.
\]
Since $\Theta^*\sim \Theta$, they induce weakly isomorphic functions. Now consider the function induced by $\Theta^*$. 
%Now consider the mapping $\mxi_\omega \mapsto I_\omega $. In the interval $I_\omega=({i_1-1\over d},\ {i_1\over d}]\times \cdots \times ({i_k-1\over d},\ {i_k\over d}]$
\begin{align}
\hat f(\mx)&=\sum_{(i_1,\ldots,i_K)} \Theta^*(i_1,\ldots,i_K) \mathds{1}\left\{ x_k\in \left({i_k-1\over d},\ {i_k\over d}\right]\right\}\\
&=\sum_{(i_1,\ldots,i_K)}f(\xi^{(1)}_{(i_1)},\ldots,\xi^{(K)}_{(i_K)})\mathds{1}\left\{ x_k\in \left({i_k-1\over d},\ {i_k\over d}\right], \text{ for all }k\in[K]\right\}.
\end{align}
We aim to evaluate the integral over the grid $I_i=({i-1\over d}, {i\over d}]$ for $i\in[d]$. 
\begin{align}\label{eq:total}
\text{Loss}(f,\ \hat f)=\sum_{(i_1,\ldots,i_K)}\left[\int_{\mx \in I_{i_1}\times \cdots \times I_{i_K}}|f(\mx)-\hat f(\mx)|^2 d\mx\right]
%&\leq 2M \sum_{(i_1,\ldots,i_K)}\left[\int_{\mx \in I_{i_1}\times \cdots \times I_{i_K}}|f(\mx)-\hat f(\mx)| d\mx\right].
\end{align}
We now evaluate the integral over the region $I_\omega=I_{i_1}\times \cdots \times I_{i_K}$.
Define $\check \mx=({i_1-1\over d}, {i_1\over d}]\times \cdots ({i_K-1\over d},{i_K-1\over d}]\in I_\omega$. For any $\mx \in I_\omega$, we have
\begin{align}
|f(\mx)-\hat f(\mx)|&\leq \left|f(\mx)-f(\check \mx)\right|+\left|\hat f(\mx)-f(\check \mx)\right|.
\end{align}
Note that both $\check \mx, \mx \in I_\omega$
\begin{equation}\label{eq:I}
|f(\mx)-f(\check \mx)|\leq L |\mx-\check\mx|^{\alpha} \leq L K d^{-\alpha}.
\end{equation}
%Therefore,
%\[
%\int_{\mx\in I_\omega} |f(\mx)-f(\check \mx)| \leq {LKd^{-\alpha} \over d^K}
%\]
Furthermore, 
\begin{align}\label{eq:II}
|\hat f(\mx)-f(\check \mx)|& \leq |f(\xi^{(1)}_{(i_1)},\ldots,\xi^{(K)}_{(i_K)}) - f({i_1\over d},\ldots,{i_K\over d})| \\
&\leq L\max_k \left|\xi^{(k)}_{(i_k)}-{i_k\over d}\right|^{\alpha}.
\end{align}
Plugging~\eqref{eq:I} and~\eqref{eq:II} to~\eqref{eq:total}, we obtain

\begin{align}
\int_{\mx\in[0,1]^K} |f(\mx)-\hat f(\mx)|^2 d\mx&\leq L^2K^2\left\{d^{-\alpha}+\max_{k,i_k} \left|\xi^{(k)}_{(i_k)}-{i_k\over d}\right|^{\alpha}\right\}^2\\
&\leq L^2K^2\left\{d^{-2\alpha}+\max_{k,i_k}\left|\xi^{(k)}_{(i_k)}-{i_k\over d}\right|^{2\alpha}+2d^{-\alpha}\max_k\left|\xi^{(k)}_{(i_k)}-{i_k\over d}\right|^{\alpha}\right\}.
\end{align}

By Jensen's inequality, $f(x)=x^{\alpha}$ is concave for $\alpha\in[0,1)$, so henece $\mathbb{E}[f(x)]\leq f(\mathbb{E}(x))$:
\[
\mathbb{E}\left|\xi^{(k)}_{(i_k)}-{i_k\over d}\right|^{2\alpha}\leq \left[\text{Var}\left(\xi^{(k)}_{(i_k)}\right)\right]^{\alpha }\leq Cd^{-\alpha}.
\]
The last line comes from the fact that the order statistics of the uniform distribution belongs to Beta distribution, $\xi_{(i)}\sim \text{Beta}(i, d+1-i)$ for all $i\in[d]$. Then $\text{Var}(\xi_{(i)})$
By $\mathbb{E}(X)\leq \sqrt{\mathbb{E}(X^2)}$, we have
\[
\mathbb{E}\left|\xi^{(k)}_{(i_k)}-{i_k\over d}\right|^{\alpha}\leq Cd^{-\alpha /2}.
\]


Therefore,
\begin{align}
\text{Loss}(f, \hat f)\leq L^2K^2\left(d^{-2\alpha}+Cd^{-\alpha}+2Cd^{-3\alpha/2}\right)\leq CL^2K^2d^{-\alpha}.
\end{align}

\end{proof}

\begin{proof} By definition, $f\in\tF(R,1)$ implies there exist a set of mappings $\phi_k \colon[0,1] \mapsto[R]$ such that
\[
f(\mx)=\sum_{\mr\in[R]^K}\tC(\mr)\mathds{1}\left\{\phi_k(x_k)=r_k,\ \text{for all }k\in[K]\right\},\ \text{for all }\mx\in[0,1]^K.
\]
Without loss of generality, assume $\phi_k^{-1}(r)=(\lambda^{(k)}_{r-1},\lambda^{(k)}_{r}]$, where $0=\lambda^{(k)}_1\leq \cdots \lambda^{(k)}_2\leq \cdots \leq \lambda^{(k)}_{R-1}\leq \lambda^{(k)}_R=1$ are a sequence of cut-off points on $[0,1]$, and the interval length preserves the Lebesgue measure $|\{x_k\in\phi^{-1}_k(r)\}|=|\lambda^{(k)}_r-\lambda_{r-1}^{(k)}|$ for all $r\in[K]$. Under this assumption, $f$ has an isomorphic form
\begin{equation}\label{eq:original}
f(\mx)=\sum_{\mr\in[R]^K}\tC(\mr)\mathds{1}\left\{x_k\in(\lambda^{(k)}_{r_k-1},\ \lambda^{k}_{r_k}],\ \text{for all }k\in[K]\right\},\ \text{for all }\mx\in[0,1]^K.
\end{equation}
Now consider $f\Rightarrow \Theta$. By definition, for all $(i_1,\ldots,i_K)\in[d_1]\times\cdots\times[d_K]$,
\[
\Theta(i_1,\ldots,i_K)=\sum_{\mr\in[R]^K}\tC(\mr)\mathds{1}\left\{\xi_{i_k}^{(k)} \in(\lambda^{(k)}_{r_k-1},\ \lambda^{(k)}_{r_k}],\ \text{for all }k\in[K]\right\},
\]
where $\{\xi^{(k)}_{i_k}\}$ are i.i.d.\ Unif[0,1] for all $i_k\in[d]$ and $k\in[K]$. 

From $\Theta\Rightarrow \hat f$, we have
\begin{align}
\hat f(\mx)&=\sum_{(i_1,\ldots,i_K)}\Theta(i_1,\ldots,i_K)\mathds{1}\left\{x_k \in \left({i_k-1\over d},\ {{i_k}\over d}\right],\ \text{for all }k\in[K] \right\}\\
&=\sum_{\mr\in[R]^K}\tC(\mr)\sum_{(i_1,\ldots,i_K)}\mathds{1}\left\{\xi^{(k)}_{i_k}\in\left(\lambda^{(k)}_{r_k-1},\ \lambda^{(k)}_{r_k}\right] \text{ and } x_k \in \left({i_k-1\over d},\ {i_k\over d}\right], \text{ for all } k\in[K]\right\}\\
&=\sum_{\mr}\tC(\mr) \prod_{k\in[K]} \left(\sum_{i_k\in[d]} \mathds{1}\left\{\xi^{(k)}_{i_k}\in\left(\lambda^{(k)}_{r_k-1},\ \lambda^{(k)}_{r_k}\right] \text{ and } x_k \in \left({i_k-1\over d},\ {i_k\over d}\right]\right\}   \right)
\end{align}
%Define $I_{\mr}=\{\mx\colon \phi(\mx)=\mr\}$, and denote $\lambda_{\mr}=|I_{\mr}|\in[0,1]$ the Lebesgue measure over the $[0,1]^K$. 
For each $k\in[K]$, define the empirical cumulative proportion of categories among the set $\{\xi^{(k)}_1,\ldots,\xi^{(k)}_d\}$
\[
\hat \lambda^{(k)}_r={1\over d}\sum_{i\in[d]}\mathds{1}\left\{\xi^{(k)}_{i}\leq \lambda^{(k)}_{r}\right\},\ \text{for all } r\in[R].
\]
The function $\hat f$ can be equivalently written as follows:
\begin{align}\label{eq:target}
\hat f(\mx)&=\sum_{\mr}\tC(\mr) \prod_{k\in[K]}\mathds{1}\left\{  x_k \in (\hat \lambda^{(k)}_{r_k-1},\ \hat \lambda^{(k)}_{r_k} ]\right\}\\
&=\sum_{\mr} \tC(\mr) \mathds{1}\{ x_k \in (\hat \lambda^{(k)}_{r_k-1},\ \hat \lambda^{(k)}_{r_k}]\text{ for all }k\in[K] \}.
\end{align}
Applying Lemma~\ref{lem:Monto} to~\eqref{eq:original} and~\eqref{eq:target}, we conclude
\[
\mathbb{E}[\text{loss}(f,\ f')]\leq {8KR\over \sqrt{d}}.
\]
\end{proof}


We partition the set $[0,1]^K$ in two ways:
\begin{enumerate}
\item Partition based on $f$.
\[
[0,1]^K=\cup \left\{ \phi^{-1}(\mr)\colon \mr\in[R]^K\right\}, \text{ where }\phi^{-1}(\mr)\stackrel{\text{def}}{=}\{\mx\in[0,1]^K\colon \phi(\mx)=\mr\}.
\]
\item Partition based on $\hat f$.
\[
[0,1]^K=\cup \left\{ h^{-1}(\mr)\colon \mr\in[R]^K\right \}, \text{ where }h^{-1}(\mr)\stackrel{\text{def}}{=}\{\mx\in[0,1]^K\colon h(\mx,\mr)=1\}.
\]
\end{enumerate}
We define a one-to-one mapping between $\phi^{-1}(\mr)$ and $h^{-1}(\mr)$. The mapping is measure-preserving in the sense that $|p_{\mr}|$ (??)


We will evaluate the difference between $\phi^{-1}(\mr)$ and $h^{-1}(\mr)$. The 
$p_{\mr}:=|\phi^{-1}(\mr)| \in[0,1]$ denote the Lebesgue measure of the set $\phi^{-1}(\mr)$. Note that $h^{-1}(\mr)$ is a random set where the randomness comes from $\mxi_{\omega}$. In particular, 
\begin{align}
\hat p_{\mr} \stackrel{\text{def}}{=}|h^{-1}(\mr)|&=|\{\mx\colon\sum_{\omega}\mathds{1}\{\phi(\mxi_{\omega})=\mr\} \mathds{1}\{\mx\in I_{\omega}\}\}|\\
&={1\over d^K}\sum_{\omega}\mathds{1}\{ \phi(\mxi_\omega)=\mr \}\\
&={1\over d^K}\sum_{(i_1,\ldots,i_K)}\left(\prod_k\mathds{1}\{\phi_k(\xi_{k,i_k})=r_k\}\right)\\
&={1\over d^K} \prod_k \left(\sum_{i_k\in[d]}\mathds{1}\{\phi_k(\xi_{k,i_k})=r_k\}\right)\\
&={1\over d^K} \prod_k \text{Bin}(d,\ \lambda_k(r_k))
\end{align}
where the event $\mathds{1}\{ \phi_k(\xi_{k,i_k})=r_k \}$ are i.i.d.\ Bernoulli random variables with success probability $\lambda_k(r_k)=|x_k\in[0,1]\colon \phi_k(x_k)=r_k|\in[0,1]$ for all $i_k \in[d]$ and $k\in[K]$. 
Therefore, 
\[
\hat p_{\mr}\sim {1\over d^K}\prod_k \text{Bin}(d, \lambda_k(r_k))\quad \text{and}\quad p_{\mr}=\prod_k \lambda_k(r_k).
\]
which implies
\[
\mathbb{E}\left(|\hat p_{\mr}-p_{\mr}|^2\right) \leq  {K p_{\mr} \over d}.
\]
Now, we evaluate the loss:
\begin{align}
\int_{\mx\in[0,1]^K}|f(\mx)-f'(\tau(\mx))|^2d\mx&\leq \sum_{\mr\in[R]^K}\int_{\mx\in\phi^{-1}(\mr)}|f(\mx)-f'(\tau(\mx))|^2d\mx\\
%&\leq 4\sum_{\mr}|\left\{ \mx\colon \phi(\mx)=\mr\text{ but }h(\mx)\neq \mr\} |  \\
%&\leq 4\sum_{\mr}|\phi^{-1}(\mr)-h^{-1}(\mr)| (?)\\
%&\leq  C \sqrt{K R^K \over d}
\end{align}


\begin{proof}
\begin{align}
\text{Loss}(f,\ f')&=\inf_{f'_{\text{iso}\sim f'}\sim f}\int_{\mx\in[0,1]^K}|f'_{\text{iso}}(\mx)-f(\mx)|^2d\mx\\
&\leq\inf_{\tau: \text{ measure-preserving map}} \int_{\mx\in[0,1]^K}|f'(\tau(\mx))-f(\mx)|^2d\mx.
\end{align}
By assumption, $f$ is piecewise constant over the gird, $\tG=\cup_iI_i$, where the intervals $I_i\subset [0,1]^K$ are disjoint for all $i\in[R^K]$. Note that
\[
\int_{\mx\in I_i} |f(\mx)-f'(\mx)|^2d\mx =4\int_{\mx\in I_i}\mathds{1}\left\{f(\mx)\neq f'(\mx)\right\}d\mx=4\mu\{\mx\in I_i\colon f(\mx)\neq f'(\mx)\},
\]
where $\mu\{\cdot\}$ denotes the Lebesgue measure. We aim to find two isomorphisms to upper bound the Lebesgue measure. Specifically, let $c=f(\mx)$ denote the constant in the interval $I_i$. We want to find the region for which $\{\mx\in I_i\colon f'(\tau(\mx))\neq c\}$ where $\tau$ is a measure-preserving map. Recall the definition $f\Rightarrow \Theta(f,\mxi)\Rightarrow f'$. 

First, we consider $f\Rightarrow \Theta(f,\mxi)$. The random tensor $\Theta=\Theta(f,\mxi)$ is expressed as
\[
\Theta(i_1,\ldots,i_K)=f(\xi_{1,i_1},\ldots,\xi_{K,i_K}),\ \text{for all }[i_1,\ldots,i_K]\in[d_1]\times \cdots \times[d_K].
\]
For each fixed $k\in[K]$, we sort the elements in $\{\xi_{k,i_k}\colon i_k\in[d_k]\}$ from smallest to largest. With a little abuse of notation, we denote $\xi_{k,1}\leq \cdots \leq \xi_{k,d_k}$ for all $k\in[K]$. The sorted tensor $\Theta^*$ is expressed as
\[
\Theta^*(i_1,\ldots,i_K)=\Theta(\sigma_1(i_1),\ldots,\sigma_K(i_K)),\ \text{for all } [i_1,\ldots,i_K]\in[d_1]\times \cdots \times[d_K],
\]
where $\sigma_k\colon[d_k]\mapsto[d_k]$ denotes the permutation that sorts $\{\xi_{k,i_k}\}_{i_k\in[d_k]}$ in increasing order. Note that $\Theta^*\sim \Theta$. By property (i), there exists $f^*\sim f$ such that $\Theta^*$ is induced by $f^*$. 
Therefore, it suffices to evaluate the loss between functions $f^*$ and $f'$. Furthermore, by property (ii), without loss of generality, we define $f'$ the function induced by $\Theta^*$. We aim to investigate the value $f'(\mx)$ for $\mx\in I$. 

Then, we consider $\Theta^* \Rightarrow f'$. By definition, for all $(x_1,\ldots,x_K)\in[0,1]^K$,
\begin{align}
f'(\mx)&=\sum_{(i_1,\ldots,i_K)} \Theta^*(i_1,\ldots,i_K)\mathds{1}\left\{\mx \in \left({i_1-1\over d_1},\ {i_1\over d_1}\right] \times \cdots \times \left({i_K-1\over d_K},\ {K\over d_K}\right] \right\},\\
&=\sum_{(i_1,\ldots,i_K)}f(\xi_{1,i_1},\ldots,\xi_{K,i_K})\mathds{1}\left\{\mx \in \left({i_1-1\over d_1},\ {i_1\over d_1}\right] \times \cdots \times \left({i_K-1\over d_K},\ {K\over d_K}\right] \right\}\\
&=\sum_{\mr}C(\mr)\mathds{1}\left\{ \phi(\mx)=\mr \right\}
\end{align}







Recall that $\Theta\in \tP(R,1)$.... Suppose $\xi_1< \cdots < \xi_d$. $\xi_k\in R$, $\xi_{k+1}\in R$,... then $d_1x_1 $ 

The only places that $f$ and $f'$ differ are
\[
|I-\hat I|,\quad \text{where }d_{\text{total}}\hat I \sim \text{Bernoulli}(d_{\text{total}}, I).
\]
Therefore
\[
\mathbb{E}(|I-\hat I|)\leq \sqrt{I\over d_{\text{total}}}.
\]
Summing over $I_i$ over $i\in[R^K]$ and note that $\sum_i I_i=1$, we have
\[
\left|\{\mx: f(\mx)\neq f'(\mx)\}\right|_{\lambda}\leq \sum_{i\in[R^K]}\mathbb{E}(|I_i-\hat I_i|)\leq \sum_{i\in[R^K]} {\sqrt {I_i} \over \sqrt{d^K}}\leq \left({R\over d}\right)^{K/2}.
\]
\end{proof}



\section{Main results}
\begin{thm} [loss in $\tF(R)$ family] Let $f\in\tF(R)$ and $\tY \sim \mathbb{P}(f, \mxi)$ generated from the tensor nonparametric model. Let $\hat f$ be the least-square estimator with pre-specified block size $R$. Then,
\[
\mathbb{E}\left[\text{Loss}(f,\hat f)\right] \leq C \left({R^K\over d^K} + {K\log R \over d^{K-1}}\right)+{8KR\over \sqrt{d}},
\]
where the expectation is taken jointly over $\tY$ and $\mxi$ (??) (uniformly over $f\in \tF(R)$??). In particular, let $\Theta=f(\mxi)$ and $\hat \Theta=\hat f(\mxi)$. Then, 
\begin{equation}\label{eq:MSE}
\mathbb{E}\left[\text{Loss}(\Theta,\hat \Theta)\right]\leq C\left({R^K\over d^K} + {\log R \over d^{K-1}}\right),
\end{equation}
where again the expectation is taken jointly over $\tY$ and $\mxi$ (? or conditinoally).
\end{thm}

\begin{proof} 
We first proof~\eqref{eq:MSE}. Recall that $\hat \Theta =\argmin_{\Theta\in\tF(R,M)}F(\Theta)$ and $\omega_{\Theta\in\tF(R,M)}\nabla^2F(\Theta)={1\over 2}$. By Taylor expansion of $F(\Theta)$ around $\hat \Theta$, we have
\begin{align}
\FnormSize{}{ \hat  \Theta - \trueT } &\leq 2\left\langle \trueT-\tY,\  {\hat \Theta-\trueT \over \FnormSize{}{\hat \Theta-\trueT}} \right\rangle\\
& \leq 2\max_{\Theta\in \tP(2R,1)}\langle \tE,\ \Theta\rangle.
\end{align}
By union bound, for any $t>0$
\begin{align}
\mathbb{P}\left(\max_{\trueT\in\tP(R,M)}\FnormSize{}{ \hat  \Theta - \trueT }\geq t\right)&\leq \sup_{\mxi} \mathbb{P}\left(\max_{\Theta\in\tP(2R,1)} \langle \tE,\ \Theta \rangle\geq {t\over 2}\big|\mxi\right)\\
 &\leq \sup_{\mxi}C|\tP(2R,1)|\exp\left( -{t^2\over 4\sigma^2}\right) \\
&\leq  C\exp(-{t^2\over \sigma^2}+Kd \log R+R^K).
\end{align}
We take $t=\sigma \sqrt{Kd\log R+R^K}$ and obtain that,
\[
\max_{\trueT\in\tP(R,M)}\FnormSize{}{ \hat  \Theta - \trueT }^2\leq \sigma^2(Kd\log R+R^K),
\] 
with probability at least $1-\exp(-Kd\log R - R^K)$, (or equivalently, uniformly over $f\in\tF(R,M)$ and $\mxi\in\mathbb{P}_{\mxi}$). Therefore,
\[
\mathbb{E}(\text{Loss}(\hat \Theta,\trueT))\leq \sigma\left( {r^K \over d^K}+{K\log r\over d^{K-1}}\right),
\]
where the expectation with respect to $\tY$ (and $\mxi$).
\end{proof}

\begin{lem}
\[
\text{Loss}(f,\hat f) \leq \text{Loss}(\Theta,\hat \Theta) +{2KR\over \sqrt{d}}.
\]
\end{lem}
\begin{proof} Define $f\Rightarrow \Theta(f,\mxi)\Rightarrow \check f$ (a random function measure-able w.r.t. $\mxi$). 
\begin{align}
\mathbb{E}\left[\text{Loss}(f,\hat f)\right]&\leq \mathbb{E}\left[\text{Loss}(\tilde f, \hat f)\right]+\mathbb{E}\left[\text{Loss}(\tilde f,  f)\right]\\
&\leq \mathbb{E}\left[\text{Loss}(\Theta, \hat \Theta)\right]+\mathbb{E}\left[\text{Loss}(\tilde f, f)\right]\\
&\leq \sigma^2\left( {K\log R\over d^{K-1}}+ {R\over d^K} \right)+{2KR\over \sqrt{d}},
\end{align}
where the expectation is over $\tE$ and $\mxi$ jointly. 
\end{proof}
\begin{rmk}
Dense region: agonistic error dominates. Sparse region: estimation error dominates... intuition?
\end{rmk}
 
 \begin{prob}[Non-parametric estimation with unobserved designs] Let $f\colon [0,1]^K\mapsto [-1,1]$ be a target function of interest. For each $k\in[K]$, we draw a random i.i.d.\ sample of $d$ points $\{ x^{(k)}_i\}_{i\in[d]}$ uniformly from $[0,1]$. Write $\omega=(i_1,\ldots,i_K)\in[d]^K$ and $\mx_\omega=(x^{(1)}_{i_1},\ldots,x^{(K)}_{i_K})^T\in\mathbb{R}^K$. The goal is to estimation $f$ given the set $\{\left(\omega, f(\mx_\omega)\right)\}$ but without knowing $\{\mx_\omega\}$! 
\end{prob}
The following lemma gives the estimation error for stepwise constant functions $f\in \tF(R,1)$.

\begin{lem}[Step function approximation with unobserved designs]\label{lem:Monto}Let $f\colon[0,1]^K\mapsto[-1,1]$ be an $R$-step function:
\[
f(\mx)=\sum_{\mr \in[R]^K} c_{\mr}\mathds{1}\left\{\mx \in \KeepStyleUnderBrace{(\lambda^{(1)}_{r_1-1},\ \lambda^{(1)}_{r_1}]\times \cdots \times  (\lambda^{(K)}_{r_K-1},\ \lambda^{(K)}_{r_K}]}_{=:I_{\mr}}\right\},\quad \text{for all }\mx\in[0,1]^K,
\]
where $\mr=(r_1,\ldots,r_K)\in[R]^K$ denotes multi-index, $\{c_{\mr}\in[-1,1]\}$ are a set of real numbers, and $0=\lambda^{(k)}_{1}\leq \lambda^{(k)}_{2}\leq \cdots \leq \lambda^{(k)}_{R-1} \leq \lambda^{(k)}_{R}=1$ are a sequence of cutoff points over $[0,1]$, for $k\in[K]$. Let $p^{(k)}_{r}=\lambda^{(k)}_{r}-\lambda^{(k)}_{r-1}$ denote the length of the $r$-th interval at the mode $k$, where $r\in[R]$ and $k\in[K]$. 

For each $k\in[K]$, draw a random i.i.d.\ sample of $d$ points $\{N^{(k)}_i\}_{i\in[d]}$ from a categorical distribution with parameter $(p^{(k)}_1,\ldots,p^{(k)}_{R})$. Let 
\[
\hat \lambda^{(k)}_{r}={1\over d}\sum_i \mathds{1}\{N^{(k)}_i\leq r\},\quad \text{for all }r\in[R]
\] denote the empirical cumulative proportions based on the $d$ trials. Consider the estimator $\hat f$:
\[
\hat f(\mx)= \sum_{\mr\in[R]^K} c_{\mr} \mathds{1}\left\{\mx \in \KeepStyleUnderBrace{(\hat \lambda^{(1)}_{r_1-1},\ \hat\lambda^{(1)}_{r_1}]\times \cdots \times  (\hat\lambda^{(K)}_{r_K-1},\ \hat\lambda^{(K)}_{r_K}]}_{=:\hat I_{\mr}}\right\},\quad \text{for all }\mx\in[0,1]^K.
\]
Then
\[
\mathbb{E}\left(\int_{\mx\in[0,1]^K}|f(\mx)-\hat f(\mx)|^2 dx\right) \leq  {8K R\over \sqrt{N}},
\]
where the expectation is taken with respect to $\{N^{(k)}_i\}$.
\end{lem}
\begin{proof}
Note that
\[
\mathbb{E}\left(\int_{\mx\in[0,1]^K}|f(\mx)-\hat f(\mx)|^2 dx\right) \leq 4\mathbb{E}\int_{\mx\in[0,1]^K}\mathds{1}\{f(\mx)\neq \hat f(\mx)\}d\mx.
\]
Therefore, it suffices to evaluate the Lebesgue measure of $\{\mx \colon f(\mx)\neq \hat f(\mx)\}$. Note that $f$ and $f'$ are the same over $I_{\mr}\cap \hat I_{\mr}$ for all $\mr$. This implies
\begin{align}
\{\mx\colon f(\mx)\neq f'(\mx)\} &\subset \cup_{\mr}\{I_{\mr}\Delta \hat I_{\mr}\} \\
&\subset \cup_{k}\cup_{r}\left\{\mx\colon x_k\in (\lambda^{(k)}_{r-1},\ \lambda^{(k)}_{r}] \Delta (\hat \lambda^{(k)}_{r-1},\ \hat \lambda^{(k)}_{r}]\right\},
\end{align}
where the operation $\Delta $ denotes the symmetric difference between two sets, and the second line comes from the property for Cartesian product of intervals.  
Then,
\begin{align}\label{eq:bound}
\left|\{ \mx\colon f(\mx)\neq f'(\mx)\}\right|&\leq \sum_{k\in[K]}\sum_{r\in[R]}\left| (\lambda^{(k)}_{r-1},\ \lambda^{(k)}_{r}] \Delta (\hat \lambda^{(k)}_{r-1},\ \hat \lambda^{(k)}_{r}]\right|\\
&\leq 2 \sum_{k\in[K]}\sum_{r\in[R]} |\lambda^{(k)}_{r}-\hat \lambda^{(k)}_{r}|
%\sum_{\mr}\left|I_{\mr}\Delta \hat I_{\mr} \right|\\
%&\leq \sum_{(r_1,\ldots,r_K)}\max_{k\in[K]} \left|(\lambda^{(k)}_{r_k-1},\ \lambda^{(k)}_{r_k}] \Delta (\hat \lambda^{(k)}_{r_k-1},\ \hat \lambda^{(k)}_{r_k}]\right|\\
%&\leq \sum_{(r_1,\ldots,r_K)}\max_{k\in[K]}\left\{|\lambda^{(k)}_{r_k-1}-\hat \lambda^{(k)}_{r_k-1}|+|\lambda^{(k)}_{r_k}-\hat \lambda^{(k)}_{r_k}|\right\}
\end{align}
Note that by definition, $d\hat \lambda_{r}^{(k)}=\sum_i\mathds{1}\left\{ N^{(k)}_i\leq r\right\}$ follows from Binomial distribution with parameters $(d,\lambda^{(k)}_{r})$. Therefore, 
\begin{equation}\label{eq:var}
\mathbb{E}|\lambda^{(k)}_{r}-\hat \lambda^{(k)}_{r}|^2={\lambda^{(k)}_r(1-\lambda^{(k)}_r)\over d}\leq {1\over d}.
\end{equation}
Plugging~\eqref{eq:var} into~\eqref{eq:bound}, we obtain
\begin{align}
\left|\{ \mx\colon f(\mx)\neq \hat f(\mx)\}\right|^2& \leq 4KR \sum_{k\in[K]}\sum_{r\in[R]}|\lambda^{(k)}_{r}-\hat \lambda^{(k)}_{r}|^2\\
&\leq {4K^2R^2 \over d}
\end{align}
Henceforth, 
\[
\mathbb{E}\left|\{ \mx\colon f(\mx)\neq \hat f(\mx)\}\right|\leq {2KR\over \sqrt{d}}.
\]
\end{proof}

\section{Minimax lower bound}
\begin{thm}[Minimax]
For stochastic tensor model, there exists a constant $C>0$ such that
\[
\inf_{\hat \Theta}\sup_{\Theta \in \tP(R)} \mathbb{P}\left\{\FnormSize{}{\hat \Theta-\Theta}> C{\sigma^2 \over p} \left(R^K+K d \log R \over d^K \right)\right\}>0.2.
\]
\end{thm}

\begin{proof}
{\it Nonparametric rate.} We construct fixed $\mM^{(k)}\in[0,1]^{d\times R}$ for all $k\in[K]$ as follows. For each $k\in[K]$, $\mM\colon[d]\mapsto[R]$ partition the set $[d]$ into $R$ equal-sized clusters, and we denote the mapping rule $\mM(i)=\lceil {iR\over d} \rceil$ for $i\in[d]$. For any binary tensor $\tC\in\{0,1\}^{r_1\times \cdots \times r_K}$, we define the core tensor
\[
\check \tC= c\sqrt{\sigma^2 r^k \over p d^k} \tC.
\]
We identify the tensor in $\{0,1\}^{r_1\times \cdots \times r_K}$ by vectors in $\{0,1\}^{r^K}$. By Lemma .., there exists some set $\Omega$ such that $|\Omega|\geq \exp\left({r^K \over 4}\right)$ and $H(\tC, \tC')\geq {r^K\over 4}$ for any $\tC\neq \tC'\in \Omega$. We consider the subspace in the original tensor induced by $\$
\end{proof}

\begin{defn}[Clustering function]
Let $M\colon[d]\mapsto[R]$ denotes a clustering function, where we use $M(i)\in\mathbb[R]$ denote the cluster label to which the $i$ was assigned. We use $M^{-1}(r)=\{d\colon M(d)=r\}\subset [d]$ to denote the set of indices that was assigned to cluster $r$. For notational convenience, we use $M\in[R]^d$ or $M\in\{0,1\}^{d\times R}$ exchangeable to denote the collection of all clustering functions that map $[d]$ to $[R]$.
\end{defn}

\begin{lem}There exists a subset $\Omega \in [R]^d$, such that $|\Omega|\geq \exp(d\log R/2)$ and
\[
H(\omega,\omega')\stackrel{\text{def}}{=}\left|\left\{d\colon \omega(d)\neq \omega'(d)\right\}\right| \geq {d \over 4}, \quad \text{for all }\omega\neq \omega'\in\Omega.
\]
\end{lem}

\begin{proof}
Define
\[
\Omega=\left\{\omega\in[R]^d\colon |\omega^{-1}(i)|={d\over R} \text{ for } i\in[R]\right\},
\]
that is, $\Omega$ is the collection of clustering functions that generates the equal-sized clusters. Given any $\omega\in \Omega$, define its $\varepsilon$-neighborhood by
\[
B(\omega,\varepsilon)=\left\{\omega'\in \Omega\colon H(\omega,\omega')\leq \varepsilon\right\}.
\]
The packing number of $\Omega$
\[
\tM(\varepsilon, \Omega, H)\geq {|\Omega|\over \max_{\omega\in\Omega}|B(\omega,\varepsilon)|}.
\]

Taking $\varepsilon ={d\over 4}$ gives
\[
|B(\omega,\varepsilon)|\leq \binom{d}{d/4}R^{d/4} \leq (4e)^{d/4}R^{d/4}\leq \exp\left( {1\over 4}d\log R\right),
\]
where we have used the inequality $\binom{d}{k} \leq ({ed\over k})^{k}$ for all $k\leq d$.
By stirling's formula
\[
|\Omega|={d! \over \left[(d/R)!\right]^{R}}\approx {\sqrt{d}{\left(d\over e\right)}^d \over \sqrt{d\over R}\left(  {d\over Re} \right)^{d}}\approx \sqrt{R}\left( R \right)^{d}\geq \exp\left( d\log R + o(d \log R)\right) \geq \exp\left({1\over 2} d\log R\right).
\]
Therefore,
\[
\tM\left({d\over 4}, \Omega,H\right)\geq \exp\left( {1\over 2}d\log R \right).
\]
\end{proof}

\newpage
In Wang et al, the MSE for the tensor block model has the following asymptotical rate,
\[
\text{MSE}(\hat \Theta, \Theta)\stackrel{\text{def}}{=}{1 \over d^K}\FnormSize{}{\Theta-\hat \Theta}^2 \asymp {R^K\over d^K}+ {K\log R\over d^{K-1}},\quad \text{as }d\to \infty \text{ while fixing $R$}.
\]
We investigate the asymptotic behavior as $R\asymp d^{\delta}$, for some $\delta\in[0,1]$. 
\[
\text{MSE}(\hat \Theta, \Theta)=
\begin{cases}
d^{-K},& R=1,\\
d^{-(K-1)},& \delta = 0 \text{ (i.e. constant $R=\tO(1))$},\\
d^{-(K-1)}\log d, &\delta \in(0,{1\over K}],\\
d^{-K(1-\delta)}, & \delta \in({1\over K},1].
\end{cases}
\]
In particular, in the matrix case when $K=2$, the asymptotic rate is $\tO(d^{-1}\log d)$ whenever $R\asymp \tO(\sqrt{d})$. This is faster than regular low-rank matrix decomposition but slower than Gao's paper. 

\end{comment}

{\bf Fact:} Let $X\in\mathbb{R}^{d}$ denote a column vector and $X^*=\begin{bmatrix}
0&X^T\\
X& 0
\end{bmatrix}\in\mathbb{R}^{(d+1)\times (d+1)}$ be the lifted matrix. 
Under general nonlinear kernels, we cannot guarantee the equal decision boundaries between $X$-trained and $X^*$-trained SMMs! 

{\bf Where goes wrong? Fact: Repeated attributes are down-weighted in SVM.} 

Consider two SVMs, one trained on $X=(x_1,x_2)^T$, and another one trained on $X^*=(x_1,x_2,x_2)^T$. It turns out the decision boundary trained by $X$ and $X^*$ are different.

Reason: The primal problems for these two SVMs are
\[
(P1)\quad \min_{\beta_1,\beta_2} \left\{\beta^2_1+\beta^2_2+C\sum_i\left[ y_i(\beta_1x_1+\beta_2x_2)\right]_{+}\right\},
\]
and
\begin{align}
(P2)&\quad \min_{\beta_1,\beta_2, \beta_3}\left\{\beta^2_1+\beta^2_2+\beta_3^2+C'\sum_i\left[ y_i(\beta_1x_1+(\beta_2+\beta_3)x_2)\right]_{+}\right\}\\
&=\min_{\beta_1,\beta_2} \left\{\beta_1^2+{1\over 2}\beta^2_2+C'\sum_i\left[y_i(\beta_1x_1+\beta_2x_2)\right]_{+} \right\}.
\end{align}
The solutions to (P1) and (P2) are usually different unless $\beta_1=0$. In particular, the repeated attribute, $x_2$, is down-weighted in the cost function (P2). 

Back to the kernel SMM problem. Under general nonlinear kernels, the attributes have different occurrences in $h(X^*)$ and $h(X)$. Therefore, the two decision boundaries might be different. Note that the two classifiers agree in the special case of linear kernels, since each attribute repeats precisely twice.


{\bf Implication:} A weighted SVM
\[
\min_{\boldsymbol{\beta}}\boldsymbol{\beta}^T[\text{Cov}(X)]^{-1}\boldsymbol{\beta}+\sum_i\left[y_i\langle \boldsymbol{\beta},\ X\rangle\right]_{+}
\]
will stabilize the contribution from repeated attributes, thereby maintaining the equivalence between $X$- and $X^*$-trained SVMs. In our context of kernel SMM, however, the unequal occurrences of attributes are actually okay. We can interpret the unequal occurrences as a prior knowledge of ``importance'' in the classifier. 


\begin{thm}[Compatibility with Kernel SVM] Let $X\in\mathbb{R}^{d}$ denote a column vector and $X^*=\begin{bmatrix}
0&X^T\\
X& 0
\end{bmatrix}\in\mathbb{R}^{(d+1)\times (d+1)}$ be the symmetrized matrix. Then, under our proposed column-wise kernels, the SMM classifier trained on $X^*$ equals the SVM classifier trained on $X$. 
\end{thm}



\begin{rmk}
The decision boundaries generated by two classifiers are the same, but the optimal objective values may not be the same. 
\end{rmk}

\begin{proof} We prove the following stronger result. 

({\bf Set-up}.) Let $X\in\mathbb{R}^{m\times n}$ be the original matrix feature,
\[
X=
\begin{bmatrix}
\vertbar &\vertbar&\vertbar&\vertbar\\
x_1&x_2&\vdots &x_n\\
\vertbar&\vertbar&\vertbar&\vertbar\\
\end{bmatrix}=
\begin{bmatrix}
 \horzbar &y^T_1&\horzbar\\
  \horzbar &y^T_2&\horzbar\\
\horzbar  &\cdots &\horzbar \\
\horzbar &y^T_m&\horzbar
\end{bmatrix},
\]
where $x_i$ and $y^T_j$ denotes the $i$-th column and $j$-th row of the matrix $X$, respectively. Let $X^*\in\mathbb{R}^{(m+n)\times (m+n)}$ be the symmetrized matrix feature. Based on the assumption (of sufficiently valid kernels), the feature mapping $h$ is applied to matrix $X^*$ in a column-wise fashion; that is,
\begin{equation}\label{eq:2}
X^*=\begin{bmatrix}
0&X^T\\
X& 0
\end{bmatrix}
\Rightarrow
h(X^*)
=\left[
\begin{array}{ccc:ccc}
&&& \vertbar&\vertbar&\vertbar\\
&{\Big 0}&&h(y_1)&\ldots&h(y_m)\\
 &&&\vertbar&\vertbar&\vertbar\\
\hdashline[2pt/2pt]
\vertbar & \vertbar &\vertbar&&&\\
h(x_1)&\ldots&h(x_n)&&{\Big 0}&\\
\vertbar & \vertbar &\vertbar&&&\\
\end{array}
\right]=:\begin{bmatrix}
0& A\\
C&0
\end{bmatrix},
\end{equation}
where, with a little abuse of notation, we have used $h(x_i)$ and $h(y_i)$ to denote the non-zero coordinates in the image vector $h((0,\ldots,0,x_i))$ and $h((0,\ldots,0,y_j))$, respectively.

({\bf Conclusion.}) Assume the set of elements in $C$ contains all elements in $A$. Then, the classifier trained on $X$ equals to the classifier trained on $X^*$.

({\bf Proof of the above conclusion.}). Because the elements in $C$ contains all elements in $A$, we rearrange the elements in $C$ into a possibly larger matrix:
\begin{equation}\label{eq:1}
C=\begin{bmatrix}
0&A^T\\
D&0
\end{bmatrix}.
\end{equation}
Such arrangement is possible by, for example, setting $D=\text{diag}(\{C\}/\{A\})$. (The element arrangement within $D$ is non-important.) To prove the equivalence between $X$- and $X^*$-trained classifiers, it suffices to prove the equivalence between
\[
h(X^*)=\begin{bmatrix}
0&0&A\\
0&A^T&0\\
D&0&0\\
\end{bmatrix},\quad \text{and}\quad 
h(X)=\begin{bmatrix}
0 & A^T\\
D&0
\end{bmatrix}\quad 
\text{(c.f.~\eqref{eq:2} and~\eqref{eq:1})}.
\]

Now we apply the same proof techniques as in 051820\_SMMK\_modification.pdf. Specifically, consider the primal problems with $h(X^*)$ and $h(X)$. The relevant linear predictors are
\[
\langle B^*,\ h(X^*)\rangle=\left \langle  \begin{bmatrix}
0&0&B^*_3\\
0&B^*_2&0\\
B^*_1&0&0
\end{bmatrix},\
\begin{bmatrix}
0&0&A\\
0&A^T&0\\
D&0&0\\
\end{bmatrix}
\right\rangle,\quad \text{and}\quad
\langle B,\ X\rangle = \left \langle 
\begin{bmatrix}
0 & B_2\\
B_1&0
\end{bmatrix},\
\begin{bmatrix}
0 & A^T\\
D&0
\end{bmatrix}
\right \rangle.
\]
The optimizations are
\begin{align}
(P1)\quad \min_{B^*_1, B^*_2, B^*_3}&\FnormSize{}{B^*_1}^2+\FnormSize{}{B^*_2}^2+\FnormSize{}{B^*_3}^2+C\sum_{i}\xi_i,\\ 
\text{s.t.} \ & .... \text{ where }\langle B^*,\ h(X^*)\rangle = \langle B^*_1, D\rangle+\langle B^*_2,A^T\rangle+\langle B^*_3^T,A\rangle\\
&\hspace{3.5cm}=\langle B^*_1, D\rangle+\langle B^*_2^T+B^*_3, A \rangle,
\end{align}
and
\begin{align}
(P2)\quad  \min_{B_1, B_2}&\FnormSize{}{B_1}^2+\FnormSize{}{B_2}^2+C\sum_{i}\xi_i,\\ 
\text{s.t.} \ & ... \text{ where } \langle B,\ h(X)\rangle = \langle B_1, D\rangle+\langle B_2,A\rangle.
\end{align}

It is easy to see that the optimal solution to (P1) is achieved at $B^*_2=B^*_3^T$ because 
\[
\FnormSize{}{B^*_2}^2+\FnormSize{}{B^*_3}^2\geq {1\over 2}\FnormSize{}{B^*_2+{B^*_3}^T}^2.
\]
With this choice, the optimizations become
\begin{align}
(P1) \quad \min_{B^*_1, B^*_2} \FnormSize{}{B^*_1}^2+{1\over 2}\FnormSize{}{B^*_2}^2 +C\sum_i\left[y_i\left(\langle B^*_1,\ D \rangle+\langle B^*_2,\ A\rangle\right)\right]_{+}.
\end{align}
and
\begin{align}
(P2)\quad \min_{B_1, B_2} \FnormSize{}{B_1}^2+\FnormSize{}{B_2}^2 +C\sum_i\left[y_i\left(\langle B_1,\ D \rangle+\langle B_2,\ A\rangle\right)\right]_{+}.
\end{align}
{\color{red}These two optimizations are different! Repeated features contribute less to (P1) compared to (P2).} 
\end{proof}


\bibliographystyle{unsrt}
\bibliography{tensor_wang}

\end{document}
