\documentclass[12pt]{article}
\usepackage{amsbsy, bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts,amscd,xspace,pifont}
\usepackage{epsfig, amsfonts,color,soul}


\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{hyperref}

\textheight 9.0 in \textwidth 6.6 in \topmargin -0.6 in
\oddsidemargin 0.0in

\parskip=.01in
\makeatletter \setcounter{page}{1}
\renewcommand{\baselinestretch} {1.0}

% --------- My packages --------------------

 \usepackage{amsmath}
 \usepackage{amsfonts, bm, hyperref}
\usepackage{pdfpages, subfigure}
\usepackage{graphicx,graphics,color}
%\usepackage{myformat}
\usepackage{enumerate}
\usepackage{ifthen}
%\newcommand{\full}{1}  
\newcommand{\full}{2} % show SOLUTIONS


% --------- Page setup ---------------------
\renewcommand{\baselinestretch}{1.3}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.2in}
\setlength{\evensidemargin}{-.2in} \topmargin -.7in



% --------- My commands --------------------
 \newcommand{\scr}{\mathscr}
 \newcommand{\vsp}{\vspace{1.5in}}
 \newcommand{\hsp}{\hspace{5cm} }
 \newcommand{\vspt}{\vspace{2.0cm}}
\setlength{\parindent}{0.2in}



%%%%%% Notations defined by Miaoyan 
\def\mr{\mathbf{r}}
\def\mx{\mathbf{x}}
\def\my{\mathbf{y}}
\def\mz{\mathbf{z}}
\def\mc{\mathbf{c}}
\def\mX{\mathbf{X}}
\def\mI{\mathbf{I}}
\def\bbeta{{\boldsymbol{\beta}}}
\def\estbeta{{\boldsymbol{\hat \beta}}}
\def\mhatx{\mathbf{\hat x}}
\def\mhaty{\mathbf{\hat y}}

\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
%%%%%%%%%%%%%%%%

\begin{document}
\vspace*{3cm}

 \Large
\begin{center}
Department of Statistics \\
University of Wisconsin, Madison\\
PhD Qualifying Exam Option B\\
12:30-4:30pm, Room 133 SMI \\
\end{center}

\normalsize

\begin{itemize}
\item There are a total of FOUR (4) problems in this exam. Please do
all FOUR (4) problems.

\item Each problem must be done in a separate exam book.

\item Please turn in FOUR (4) exam books.

\item Please write your code name and \textbf{NOT} your real name on
each exam book.

\end{itemize}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\setcounter{enumi}{0}

\item Consider a linear regression model with $p+1$ predictors (including intercept):
\[
Y=\beta_0+\beta_1X_1+\cdots+\beta_{p}X_{p}+\varepsilon,
\]
where $\varepsilon\sim \mathcal{N}(0,\sigma^2)$. Suppose $\sigma^2$ is unknown and consider $n$ independent observations from the model. Let $\my=(y_1,\ldots,y_n)^T$ denote the observed responses; let $\mX=[\mx_0,\ldots,\mx_p]$ denote the $n\times (p+1)$ design matrix, where $\mx_i$ corresponds to the $(i_1+1)^{\text{th}}$ column of $\mX$; and let $\bbeta=(\beta_0,\ldots,\beta_p)^T$ denote the vector of regression coefficients. Assume that $\text{rank}(\mX)=p+1$. Let $\hat \beta_j$ denote the least squares estimate of $\beta_j$, and define $\estbeta=(\hat \beta_0,\ldots,\hat \beta_{p})^T$.

\begin{enumerate}
\item Suppose $\mc=(c_0,\ldots,c_p)^T$ is a vector of known constants. Consider the hypothesis testing:
\[
\mathcal{H}_0:\sum_{j=0}^p c_i \beta_j=h\quad \text{vs.}\quad \mathcal{H}_1: \sum_{j=0}^p c_j \beta_j\neq h,
\]
where $h$ is a given constant. Explain how to test the hypothesis at significance level $\alpha$. Construct a suitable test statistics, find its distribution, and specify the rejection region.

{\color{red}Solution: We first investigate the distribution of $\mc^T\estbeta$. Based on the properties of least squares estimation, we have
\begin{equation}\label{eq:beta}
\estbeta \sim \mathcal{MVN}\left(\bbeta, \sigma^2\left(\mX^T\mX\right)^{-1}\right).
\end{equation}
Therefore, $\mc^T\estbeta\sim \mathcal{N}(\mc^T\bbeta, \sigma^2\mc^T \left(\mX^T\mX\right)^{-1}\mc)$. To test the hypothesis $\mathcal{H}_0\colon \mc^T\bbeta=h$ vs. $\mathcal{H}_1 \colon \mc^T\bbeta\neq h$, we propose the following test statistic:
\[
T=\frac{\mc^T\estbeta  - h} {\hat \sigma\sqrt{\mc^T \left(\mX^T\mX\right)^{-1}\mc}},
\]
where $\hat \sigma = \sqrt{\frac{1}{n-p-1}\sum_{i=1}^n(y_i-\hat y_i)^2}$ is the estimator of $\sigma$. Under the null hypothesis, the test statistic $T$ follows a $t$-distribution with $(n-p-1)$ degrees of freedom. The rejection region is given by $ \{ T: |T|\geq t_{n-p-1, \alpha/2}\}$,
where $t_{n-p-1,\alpha/2}$ is the $t$-critical value with $(n-p-1)$ degrees of freedom. Equivalently, the $(1-\alpha)$-CI for $\mc^T\bbeta$ is 
\[
\mc^T\estbeta\pm t_{n-p-1,\alpha/2} \hat \sigma \sqrt{\mc^T \left(\mX^T\mX\right)^{-1}\mc}.
\]
}
\item Suppose we wish to predict the value of a future observation $Y_{n+1}$. Let $\mz=(1,z_1,\ldots,z_p)^T$ denote its corresponding vector of predictor variables (i.e., $X_i=z_i$, for $i=1,\ldots,p$), and consider the prediction $\hat Y_{n+1}=\mz^T\estbeta$. Find the distribution of $Y_{n+1}-\hat Y_{n+1}$. 
%\item Show that the mean squared error (MSE) of the prediction $\hat Y_{n+1}=\mathbf{z}^T\boldsymbol{\hat \beta}$ is strictly greater than $\sigma^2$.

{\color{red}
Solution: Recall that $\hat Y_{n+1}=\mz^T\estbeta $ and $Y_{n+1}=\mz^T\bbeta+\varepsilon_{n+1}$, where $\varepsilon_{n+1}\sim N(0,\sigma^2)$. Because $\varepsilon_{n+1}$ is independent of $\varepsilon_{i}$ for all $i=1,\ldots,n$, $\hat Y_{n+1}$ is independent of $Y_{n+1}$. 
Based on the property~\eqref{eq:beta}, we have
\[
\hat Y_{n+1}\sim \mathcal{N}(\mz^T\bbeta,\ \sigma^2\mz^T\left(\mX^T\mX\right)^{-1}\mz)\quad \text{and}\quad Y_{n+1}\sim \mathcal{N}(\mz^T\bbeta,\ \sigma^2).
\]
Therefore,
 \[
Y_{n+1}-\hat Y_{n+1}\sim \mathcal{N}\left(0,\sigma^2 \mz^T\left(\mX^T\mX\right)^{-1}\mz+\sigma^2 \right).
\]
}
\item Given the vector $\mz^T$ of predictor variables for the future observation $Y_{n+1}$, find an interval $I$ such that $\mathbb{P}(Y_{n+1}\in I)=1-\alpha$. 
 
 {\color{red}
 Solution: Based on the part (b), we have 
 \[
 \frac{Y_{n+1}-\hat Y_{n+1}}{\hat \sigma \sqrt{\mz^T\left(\mX^T\mX\right)^{-1}\mz+1}} \sim T_{n-p-1}.
 \]
Therefore, the $(1-\alpha)$-CI for $Y_{n+1}$ is
\[
\hat Y_{n+1}\pm t_{n-p-1,\alpha/2} \hat \sigma \sqrt{\mz^T\left(\mX^T\mX\right)^{-1}\mz+1},
\] 
where $t_{n-p-1,\alpha/2}$ is the critical value for $t$-distribution with $(n-p-1)$ degrees of freedom. 
 }
 
 \item Suppose an additional predictor variable $X_{p+1}$ is added to the model to obtain
 \[
 Y=\gamma_0+\gamma_1X_1+\cdots +\gamma_{p+1}X_{p+1}+\varepsilon.
 \]
Suppose that the augmented design matrix $ \mathbf{\tilde X}=[\mX, \mX_{p+1}]$ has rank $p+2$. Let $\mhatx_{p+1}=a_0\mx_0+\cdots+a_{p}\mx_{p}$ denote the least-squares projection of $\mx_{p+1}$ onto the subspace of $\mathbb{R}^n$ spanned by the columns of $\mX$. Define the residual vector $\mathbf{r}_{p+1}=\mx_{p+1}-\mhatx_{p+1}$ in terms of $\mX$ and $\mx_{p+1}$.

{\color{red}
Solution: The least-squares projection of $\mx_{p+1}$ can be obtained by regressing $\mx_{p+1}$ against the columns of $\mX$. Specifically, 
\[
\mhatx_{p+1}= \mX(\mX^T\mX)^{-1}\mX^T\mx_{p+1},
\]
where $\mX(\mX^T\mX)^{-1}\mX^T$ is the projection matrix (i.e., ``hat'' matrix) that maps the vector in $\mathbb{R}^n$ to the subspace $\text{Span}(\mX)$. It follows that
\[
\mx_{p+1}-\mhatx_{p+1}=\mx_{p+1}-\mX(\mX^T\mX)^{-1}\mX^T\mx_{p+1}=\left(\mI-\mX(\mX^T\mX)^{-1}\mX^T\right)\mx_{p+1},
\]
where $\mI$ denotes the $n$-by-$n$ identity matrix. 
}

\item Express the least squares estimates $\hat \gamma_0,\ldots,\hat \gamma_{p+1}$ in terms of only $\hat \beta_0,\ldots,\hat \beta_p,a_0,\ldots,a_p,\mr_{p+1}$ and $\mathbf{y}$.

{\color{red}
Let $\mhaty$ denote the projection of $\my$ onto the column space of $\tilde \mX$. Then
\begin{equation}\label{eq:full}
\mhaty=\hat \gamma_0+\hat \gamma_1\mx_1+\cdots+\hat \gamma_p\mx_p+ \hat \gamma_{p+1}\mx_{p+1}.
\end{equation}
Note that we have
\begin{equation}\label{eq:subspace}
\mx_{p+1}=\hat \mx_{p+1}+\mr_{p+1},
\end{equation}
where $\hat \mx_{p+1}$ falls in $\text{Span}(\mX)$ and $\mr_{p+1}$ is orthogonal to $\text{Span}(\mX)$.
Combining equations~\eqref{eq:full} and~\eqref{eq:subspace} yields
\begin{align}
\mhaty&=\hat \gamma_0+\hat \gamma_1\mx_1+\cdots+\hat \gamma_p\mx_p+ \hat \gamma_{p+1}\hat \mx_{p+1}+\hat \gamma_{p+1}\mr_{p+1}\notag\\
&=\KeepStyleUnderBrace{(\hat \gamma_0+a_0)+(\hat \gamma_1+a_1)\mx_1+\cdots +(\hat \gamma_p+a_p)\mx_p}_{\text{Span}(\mX)}+\KeepStyleUnderBrace{\hat \gamma_{p+1}\mr_{p+1}}_{\text{Span}(\mX)^{\perp}}\notag,
\end{align}
where the last line comes from the fact that $\hat \mx_{p+1}=a_0\mx_0+\ldots+a_p\mx_p$.

\centerline{\includegraphics[width=10cm]{projection.pdf}}

On the other hand, the coefficient $\estbeta$ corresponds to the projection of $\my$ onto the subspace $\text{Span}(\mX)$. Hence
\[
\hat \gamma_0+a_0=\hat \beta_0,\quad \hat \gamma_1+a_1=\hat \beta_1,\quad \ldots, \quad \hat \gamma_p+a_p=\hat \beta_p,
\]
and
\[
\hat \gamma_{p+1}=\frac{\my^T\mr_{p+1}}{ \mr^T_{p+1}\mr_{p+1}}.
\]
The above equations can be summarized as
\[
\hat \gamma_i =
\begin{cases}
\hat \beta_i-a_i,& \text{if } i=0,\ldots,p,\\
\frac{\my^T\mr_{p+1}}{ \mr^T_{p+1}\mr_{p+1}}, & \text{if } i=p+1.
\end{cases}
\]
}
\end{enumerate}
\end{enumerate}

\clearpage

%%%%%%%%%%%%%%%

\end{document}
