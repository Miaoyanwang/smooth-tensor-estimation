\relax 
\providecommand\hyper@newdestlabel[2]{}
\@ifundefined{amsrefs@bibcite}{}{\let\bibcite\amsrefs@bibcite}
\bibstyle{amsrn}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\citation{wang2019three}
\citation{wang2018learning}
\citation{pmlr-v119-lee20i}
\citation{bruno2011multimodal}
\citation{timpson2018genetic}
\citation{bruno2011multimodal}
\citation{timpson2018genetic}
\citation{belkin2019reconciling}
\citation{NEURIPS2020_f9d3a954}
\citation{ge2020optimization}
\@writefile{toc}{\contentsline {section}{\numberline {1}Research Goals and Significance}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) GTEx collects gene expression profiles of over 20,000 genes from 544 individuals across 53 human tissues. (b) HCP collects multimodal imaging data from over 1,200 individuals. Figures are reproduced based on\nobreakspace  {}\cite {timpson2018genetic, bruno2011multimodal}.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro}{{1}{1}{(a) GTEx collects gene expression profiles of over 20,000 genes from 544 individuals across 53 human tissues. (b) HCP collects multimodal imaging data from over 1,200 individuals. Figures are reproduced based on~\cite {timpson2018genetic, bruno2011multimodal}.\relax }{figure.caption.1}{}}
\citation{murdoch2019definitions}
\citation{young2018universality}
\citation{wang2018learning}
\citation{pananjady2020isotonic}
\citation{kolda2009tensor}
\citation{de2000multilinear}
\citation{ganti2015matrix}
\citation{wang2019multiway}
\citation{wang3}
\citation{pmlr-v119-lee20i}
\citation{wang4}
\citation{wang1}
\citation{wang2019three}
\citation{wang5}
\citation{wang9}
\citation{wang6}
\citation{wang10}
\citation{wang2018learning}
\citation{wang7}
\citation{wang8}
\citation{han2020exact}
\citation{wang2}
\citation{wang11}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {section}{\numberline {2}AIM 1.\ Parametric Tensor Models with Statistical and Computational Optimality}{3}{section.2}}
\newlabel{sec:theme1}{{2}{3}{AIM 1.\ Parametric Tensor Models with Statistical and Computational Optimality}{section.2}{}}
\citation{wang4}
\citation{zhou2013tensor}
\citation{pmlr-v119-lee20i}
\citation{gandy2011tensor}
\citation{wang7}
\citation{zhang2018tensor}
\citation{han2020optimal}
\citation{wang2019three}
\citation{mele2015human}
\citation{han2020exact}
\citation{faskowitz2018weighted}
\citation{han2020exact}
\citation{faskowitz2018weighted}
\citation{lei2019consistentcommunity}
\citation{wang3}
\citation{wang4}
\citation{sun2015provable}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivation}{4}{subsection.2.1}}
\newlabel{eq:Tucker-low-rank}{{2.1}{4}{Motivation}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) Tensor block model generalizes usual matrix block model and is useful for higher-order clustering\nobreakspace  {}\cite {han2020exact}. (b) Examples of common social connectivity networks\nobreakspace  {}\cite {faskowitz2018weighted}. (c) Representation of connectivity edges in the usual stochastic (matrix) block model.\relax }}{4}{figure.caption.2}}
\newlabel{fig:1}{{2}{4}{(a) Tensor block model generalizes usual matrix block model and is useful for higher-order clustering~\cite {han2020exact}. (b) Examples of common social connectivity networks~\cite {faskowitz2018weighted}. (c) Representation of connectivity edges in the usual stochastic (matrix) block model.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Multiway Tensor Block Models}{5}{subsection.2.2}}
\newlabel{eq:model-tensor-entry}{{1}{5}{Multiway Tensor Block Models}{equation.2.1}{}}
\MT@newlabel{eq:model-tensor-entry}
\newlabel{eq:model-tensor}{{2}{5}{Multiway Tensor Block Models}{equation.2.2}{}}
\MT@newlabel{eq:model-tensor-entry}
\MT@newlabel{eq:model-tensor}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Proposed Agenda: Statistical and Computational Phase Transition}{5}{subsection.2.3}}
\MT@newlabel{eq:model-tensor-entry}
\MT@newlabel{eq:model-tensor}
\citation{wang2019multiway}
\citation{ma2015computational}
\citation{richard2014statistical}
\newlabel{thm:informal}{{2.1}{6}{}{thm.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (a) Three SNR regimes of statistical and computational behaviors for the order-$K$ dimensional-$d$ tensor block problems. A statistical-computational gap arises only for tensors of order 3 or greater. (b) Simulation confirmation of the discovered gap.\relax }}{6}{figure.caption.3}}
\newlabel{fig:phase-transition}{{3}{6}{(a) Three SNR regimes of statistical and computational behaviors for the order-$K$ dimensional-$d$ tensor block problems. A statistical-computational gap arises only for tensors of order 3 or greater. (b) Simulation confirmation of the discovered gap.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Preliminary Results and Assessing Mechanisms}{6}{subsection.2.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Multiway clustering based on tensor block model\relax }}{6}{algorithm.1}}
\newlabel{alg:B}{{1}{6}{Multiway clustering based on tensor block model\relax }{algorithm.1}{}}
\citation{wang2018learning}
\citation{pmlr-v119-lee20i}
\citation{han2020optimal}
\newlabel{nonconvexity}{{2.2}{7}{Statistical and computational accuracy}{thm.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (a) Optimization landscape of tensor block model. (b). Application of higher-order clustering to flight routes data.\relax }}{7}{figure.caption.4}}
\newlabel{fig:app}{{4}{7}{(a) Optimization landscape of tensor block model. (b). Application of higher-order clustering to flight routes data.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}AIM 2. Beyond Low-rankness: Nonparametric Estimation for High-rank Tensors}{7}{section.3}}
\newlabel{sec:theme2}{{3}{7}{AIM 2. Beyond Low-rankness: Nonparametric Estimation for High-rank Tensors}{section.3}{}}
\citation{chan2014consistent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivation}{8}{subsection.3.1}}
\newlabel{sec:intro}{{3.1}{8}{Motivation}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces High-rank tensor examples. (a) Numerical rank of tensor $g(\mathcal  {Z})$ versus $c$ in the transformation. (b) Heatmap of a full-rank matrix $\Theta \in \mathbb  {R}^{d\times d}$ with $(i,j)$-th entry equal to $\qopname  \relax o{log}(1+\qopname  \relax o{log}(i,j))$. (c) Top $d = 30$ tensor singular values in the second example. \relax }}{8}{figure.caption.5}}
\newlabel{fig:example}{{5}{8}{High-rank tensor examples. (a) Numerical rank of tensor $g(\tZ )$ versus $c$ in the transformation. (b) Heatmap of a full-rank matrix $\Theta \in \mathbb {R}^{d\times d}$ with $(i,j)$-th entry equal to $\log (1+\log (i,j))$. (c) Top $d = 30$ tensor singular values in the second example. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Proposed Agenda: Nonparametric Sign Representable Tensor Models}{8}{subsection.3.2}}
\newlabel{eq:nonparametric}{{3}{9}{Proposed Agenda: Nonparametric Sign Representable Tensor Models}{equation.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of our nonparametric methods based on based on Example\nobreakspace  {}\ref  {eq:example} in Section\nobreakspace  {}\ref  {sec:model}. (a): a noisy, incomplete tensor input. (b)-(c): Estimation of sign tensor series $\textup  {sgn}(\Theta -\pi )$ for $\pi \in \{-1,\ldots  ,-{1/ H},0,{1/H},\ldots  ,1\}$. (d): recovered signal $\mathaccentV {hat}05E\Theta $. \relax }}{9}{figure.caption.6}}
\newlabel{fig:demo}{{6}{9}{Illustration of our nonparametric methods based on based on Example~\ref {eq:example} in Section~\ref {sec:model}. (a): a noisy, incomplete tensor input. (b)-(c): Estimation of sign tensor series $\sign (\Theta -\pi )$ for $\pi \in \{-1,\ldots ,-{1/ H},0,{1/H},\ldots ,1\}$. (d): recovered signal $\hat \Theta $. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Preliminary Results}{9}{subsection.3.3}}
\newlabel{sec:model}{{3.3}{9}{Preliminary Results}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces (a)-(b) Functions $G(\pi )$ with local H\"older smoothness index $\alpha = 1$; (c) $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue), except for a total number of $|\mathcal  {N}| = r$ jump points (in red). Here $|\mathcal  {N}|$ denotes the number of jump points in $G$.\relax }}{9}{figure.caption.7}}
\newlabel{fig:cdf}{{7}{9}{(a)-(b) Functions $G(\pi )$ with local H\"older smoothness index $\alpha = 1$; (c) $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue), except for a total number of $|\tN | = r$ jump points (in red). Here $|\tN |$ denotes the number of jump points in $G$.\relax }{figure.caption.7}{}}
\citation{wang2019multiway}
\citation{chi2020provable}
\citation{wang2018learning}
\citation{hong2020generalized}
\citation{robinson1988root}
\citation{balabdaoui2019least}
\citation{ganti2015matrix}
\citation{wang2019multiway}
\citation{pmlr-v119-lee20i}
\citation{ganti2015matrix}
\MT@newlabel{eq:nonparametric}
\newlabel{eq:bound2}{{3.1}{10}{L\underline {W}, on-going work}{thm.3.1}{}}
\newlabel{eq:example}{{3.4}{10}{Structured tensors with repeating entries}{example.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of statistical convergence rate between our nonparametric tensor models and previous methods.\relax }}{10}{table.caption.8}}
\newlabel{compare}{{1}{10}{Comparison of statistical convergence rate between our nonparametric tensor models and previous methods.\relax }{table.caption.8}{}}
\citation{wang2017bayesian}
\citation{globerson2007euclidean}
\citation{wang2017bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Assessing Mechanisms}{11}{subsection.3.4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Prediction comparison between our method ($H=20$) and low-rank CP method (CPT) in the two data applications. Standard errors are in parenthesis.\relax }}{11}{table.2}}
\newlabel{tab:data}{{2}{11}{Prediction comparison between our method ($H=20$) and low-rank CP method (CPT) in the two data applications. Standard errors are in parenthesis.\relax }{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces (a) top IQ-associated edges in the brain connectivity data.   (b) top (authors, words, year) triplets in the NIPS data. \relax }}{11}{figure.8}}
\newlabel{fig:signal}{{8}{11}{(a) top IQ-associated edges in the brain connectivity data. \\ (b) top (authors, words, year) triplets in the NIPS data. \relax }{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}AIM 3: Predictive Tensor Models with Auxiliary Information}{11}{section.4}}
\newlabel{sec:theme3}{{4}{11}{AIM 3: Predictive Tensor Models with Auxiliary Information}{section.4}{}}
\citation{zhang2018network}
\citation{gabriel1998generalised}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Examples of predictive tensor models with auxiliary information. (a) Network regression with auxiliary information. (b) Spatial-temporal growth data represented as an order-3 tensor $\mathcal  {Y}$, where feature matrices $\bm  {X}_k$ are on each of the three modes.\relax }}{12}{figure.caption.9}}
\newlabel{fig:intro1}{{9}{12}{Examples of predictive tensor models with auxiliary information. (a) Network regression with auxiliary information. (b) Spatial-temporal growth data represented as an order-3 tensor $\tY $, where feature matrices $\mX _k$ are on each of the three modes.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Three Motivating Examples}{12}{subsection.4.1}}
\newlabel{ex:3}{{4.1}{12}{Dyadic data with node attributes}{example.4.1}{}}
\newlabel{eq:edge}{{4.1}{12}{Dyadic data with node attributes}{example.4.1}{}}
\newlabel{example:brain}{{4.2}{12}{Network regression model}{example.4.2}{}}
\newlabel{ex:1}{{4.3}{12}{Spatio-temporal growth model}{example.4.3}{}}
\newlabel{eq:time}{{4.3}{12}{Spatio-temporal growth model}{example.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Proposed Agenda: Tensor Neural Network Models}{13}{subsection.4.2}}
\newlabel{eq:NN}{{4}{13}{Proposed Agenda: Tensor Neural Network Models}{equation.4.4}{}}
\MT@newlabel{eq:NN}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Proposed tensor neural network model for predictive tensor learning tasks.\relax }}{13}{figure.caption.10}}
\newlabel{fig:proposal}{{10}{13}{Proposed tensor neural network model for predictive tensor learning tasks.\relax }{figure.caption.10}{}}
\MT@newlabel{eq:NN}
\citation{wang11}
\citation{wang12}
\@writefile{toc}{\contentsline {section}{\numberline {5}Integration of Education and Research}{14}{section.5}}
\newlabel{sec:impact}{{5}{14}{Integration of Education and Research}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Planned deliverables and milestones in my proposal.\relax }}{14}{figure.caption.11}}
\newlabel{fig:proposal}{{11}{14}{Planned deliverables and milestones in my proposal.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Education and Diversity}{14}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Nurturing the Next Generation of Statisticians}{15}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Broader Impacts}{15}{subsection.5.3}}
\bibcite{wang2019three}{{1}{}}
\bibcite{wang2018learning}{{2}{}}
\bibcite{wang2019multiway}{{3}{}}
\bibcite{wang5}{{4}{}}
\bibcite{wang7}{{5}{}}
\bibcite{wang8}{{6}{}}
\bibcite{wang9}{{7}{}}
\bibcite{wang3}{{8}{}}
\bibcite{han2020exact}{{9}{}}
\bibcite{pmlr-v119-lee20i}{{10}{}}
\bibcite{wang4}{{11}{}}
\bibcite{wang6}{{12}{}}
\bibcite{wang1}{{13}{}}
\bibcite{wang2}{{14}{}}
\bibcite{wang10}{{15}{}}
\bibcite{wang11}{{16}{}}
\bibcite{wang12}{{17}{}}
\bibcite{balabdaoui2019least}{{18}{}}
\bibcite{belkin2019reconciling}{{19}{}}
\bibcite{bruno2011multimodal}{{20}{}}
\bibcite{chan2014consistent}{{21}{}}
\bibcite{chi2020provable}{{22}{}}
\bibcite{de2000multilinear}{{23}{}}
\bibcite{faskowitz2018weighted}{{24}{}}
\bibcite{gabriel1998generalised}{{25}{}}
\bibcite{gandy2011tensor}{{26}{}}
\bibcite{ganti2015matrix}{{27}{}}
\bibcite{ge2020optimization}{{28}{}}
\bibcite{ghadermarzy2018learning}{{29}{}}
\bibcite{globerson2007euclidean}{{30}{}}
\bibcite{han2020optimal}{{31}{}}
\bibcite{hong2020generalized}{{32}{}}
\bibcite{kolda2009tensor}{{33}{}}
\bibcite{lei2019consistentcommunity}{{34}{}}
\bibcite{mele2015human}{{35}{}}
\bibcite{ma2015computational}{{36}{}}
\bibcite{murdoch2019definitions}{{37}{}}
\bibcite{pananjady2020isotonic}{{38}{}}
\bibcite{richard2014statistical}{{39}{}}
\bibcite{robinson1988root}{{40}{}}
\bibcite{sun2015provable}{{41}{}}
\bibcite{timpson2018genetic}{{42}{}}
\bibcite{NEURIPS2020_f9d3a954}{{43}{}}
\bibcite{wang2017bayesian}{{44}{}}
\bibcite{young2018universality}{{45}{}}
\bibcite{zhou2013tensor}{{46}{}}
\bibcite{zhang2018network}{{47}{}}
\bibcite{zhang2018tensor}{{48}{}}
\newlabel{[bibenv:1]}{18.24258pt}
