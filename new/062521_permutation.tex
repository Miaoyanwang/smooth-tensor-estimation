\documentclass[10pt]{article}
\usepackage{setspace}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{fancybox}
\usepackage{algorithm, algpseudocode}
\usepackage{url}

\usepackage{multirow}
\usepackage{color}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{comment}
\usepackage{bm}
\usepackage{enumitem}
\usepackage[breakable, theorems, skins]{tcolorbox}
\DeclareRobustCommand{\mybox}[2][gray!20]{%
\begin{tcolorbox}[   %% Adjust the following parameters at will.
        breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\dimexpr\textwidth\relax, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=0pt,outer arc=0pt,
        ]
        #2
\end{tcolorbox}
}


\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}


\usepackage[margin=1in]{geometry}
 
\allowdisplaybreaks[4]
\usepackage{bbm}


\usepackage{amsrefs}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}


\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor = blue,
    linkcolor=blue,
    filecolor=magenta,           
    urlcolor=black,
}

\usepackage[breakable, theorems, skins]{tcolorbox}
\DeclareRobustCommand{\mybox}[2][gray!20]{%
\begin{tcolorbox}[   %% Adjust the following parameters at will.
        breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\dimexpr\textwidth\relax, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=0pt,outer arc=0pt,
        ]
        #2
\end{tcolorbox}
}

\def\sign{\textup{sgn}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{model}{Model}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}[section]

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\usepackage{dsfont}

\newcommand{\hdeg}{\widehat{\textup{deg}}}


\theoremstyle{definition}
\newtheorem{open}[]{Open problems}

 \usepackage[parfill]{parskip}

\usepackage[compact]{titlesec}

\usepackage{sectsty}
\sectionfont{\fontsize{11}{10}\selectfont}
\subsectionfont{\fontsize{10.5}{10}\selectfont}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*0}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}

\input macros.tex

\begin{document}
\begin{center}
%{\bf \large High-dimensional Tensor Learning: The Good, the Bad, and the Pragmatic}\\
{\bf \large Polynomial-time estimation of permutation equivarant tensors}\\

{Miaoyan Wang, June 24, 2021}
\end{center}
\section{Permutation equivarant tensor model}
Given a symmetric tensor $\Theta\in\mathbb{R}^{d\times \cdots\times d}$ and a permutation $\sigma\colon[d]\to[d]$, we use $\Theta\circ\sigma$ to denote the permuted tensor
\[
(\Theta \circ \sigma)(i_1,\ldots,i_m)=\Theta(\sigma(i_1),\ldots,\sigma(i_m)), \quad \text{for all }(i_1,\ldots, i_m)\in[d]^m.
\]


\begin{defn}[Lipschitz smooth tensor]
\begin{align}\label{eq:smooth}
\tP(L)=\left\{ \Theta\colon \ |\Theta(\omega)-\Theta(\omega')|\leq  {L|\omega-\omega'|_{1}\over d}, \ \text{for all }\omega, \omega'\in[d]^m\right\}.
\end{align}
For simplicity, we consider $L=1$ thoughout this note. 
\end{defn}
\begin{model}[Permutated tensor model]
Let $\tY\in\mathbb{R}^{d\times \cdots \times d}$ be a data tensor generated from the model
\begin{equation}\label{eq:model}
\tY=\Theta\circ \sigma +\tE
\end{equation}
where $\Theta\in\tP(L)$ is an unknown structured tensor, $\sigma\colon[d]\to[d]$ is an unknown permutation, and $\tE \in\mathbb{R}^{d\times \cdots \times d}$ is a noise tensor consisting of zero-mean standard normal entries. 
\end{model}

\begin{rmk}[Random design vs.\ fixed design] Our results below assume no randomness in the signal tensor $\Theta$. This is the major distinction between our model and the hypergraphon model. In the graphon model, the data tensor $\tY$ has two randomnesses: one from the noise tensor $\tE$, and the other from signal tensor $\Theta$, 
\begin{equation}\label{eq:random}
\Theta(i_1,\ldots,i_m)=f(U_{i_1}, \ldots,U_{i_m}), \quad \text{with }(U_{i_\ell})_{\ell\in[m]}\sim_{\text{i.i.d}} \text{Unif}[0,1]. 
\end{equation}
We refer to~\eqref{eq:random} as the random design, and refer to the grid samples
\begin{align}\label{eq:fix}
\Theta(i_1,\ldots,i_m)=f\left({i_1\over d},\ldots,{i_m\over d}\right), \quad \text{for all }(i_1,\ldots,i_m)\in[d]^m,
\end{align}
as the fixed design. Our permutation equivarant tensor model specified by~\eqref{eq:smooth} and~\eqref{eq:model} is equivalent to classical hypergraphon model with fixed design~\eqref{eq:fix}. 
\end{rmk}


\begin{assumption}[Degree-identifiable tensors]\label{ass:degree}
Define the degree function deg$(\cdot)$ associated with a tensor $\Theta$,
\begin{align}
\text{deg}\colon[d] & \to \mathbb{R}\\
i & \mapsto {1\over d^{m-1}}\sum_{i_1,\ldots,i_m} \Theta(i_1,\ldots,i_m)\mathds{1}(i_1=i)
\end{align}
We call a smooth tensor $\Theta\in\tP(L)$ is degree-identifiable, if there exists a constant $\beta\in[0,1]$ and a small tolerance $\varepsilon_d\lesssim d^{-(m-1)/2}$ such that
\begin{equation}\label{eq:mono}
\text{deg}(i)-\text{deg}(j)\gtrsim \left({i-j\over d}\right)^{1/\beta}-\varepsilon_d, \quad \text{for all }i\geq j \in[d].
\end{equation}
\end{assumption}
\begin{rmk} 
The condition~\eqref{eq:mono} assumes the polynomial growth of population degree function up to a small error. The tolerance $\tO(d^{-(m-1)/2})$ allows for small fluctuations within statistical accuracy. We call $\beta$ the signal level, because it quantifies the identifiability of permutation from the degree. A lower value  of $\beta$ implies flatness of the function. We make the convention that a constant degree function is $0$-monotonic. 
\end{rmk}
{\bf Polynomial-time algorithm for estimating $\Theta$:}
 Input: $\tY$, $k$; Output: $\hat \sigma$ and $\hat \Theta_{\text{LS}}$. 
\begin{enumerate}
\item Sorting: Sort the nodes based on the empirical degree of $\tY$. The sorting returns the node permutation $\hat \sigma\colon[d]\to[d]$ for which the degree function associated with $\tY\circ \hat \sigma^{-1}$ is non-decreasing in $i\in[d]$. 
\item Blocking: Estimate $\Theta$ based on block tensor approximation
\[
\hat \Theta_{\text{LS}}=\text{Block}_{k}(\tY\circ \hat \sigma),
\]
where the operator $\text{Block}_k(\cdot)$ converts a tensor to a block tensor with $k$ equal-sized blocks; i.e,
\begin{align}\label{eq:est}
\hat \Theta_{\text{LS}}(\omega'):=\text{Block}_{k}(\tY\circ \hat \sigma)(\omega')=\text{Average}\left\{\Theta(\omega)\colon \lfloor{\omega k/d}\rfloor = \lfloor{\omega' k/d}\rfloor \right\}, \quad \text{for all }\omega'\in[d]^m.
\end{align}
\end{enumerate}
We quantify the estimation error using risk
\[
\tR(\hat \Theta, \Theta)={1\over d^m}\mathbb{E}_{\tY}\FnormSize{}{\hat \Theta-\Theta}^2.
\]


\begin{thm}[Sorting-and-blocking under $\beta$-monotonicity of degree]\label{thm:main}
Consider model~\ref{eq:model} under Assumption~\ref{ass:degree}. Set $k=d^{m\over 2+m}$ in Algorithm 1. Then, with probability at least $1-d^{-1}$, 
\[
\tR(\hat \Theta_{\text{LS}}, \Theta) \leq \KeepStyleUnderBrace{d^{-{2m\over 2+m}}}_{\text{statistical error}}+\KeepStyleUnderBrace{d^{-{\beta(m-1)}}}_{\text{algorithmic error}}. 
\]
\end{thm}

\begin{rmk}
When $\beta\geq {2m\over (m-1)(m+2)}$, the statistical error dominates the algorithmic error.  In this regime, we have 
\[
\tR(\hat \Theta_{\text{LS}}, \Theta) \leq d^{-{2m\over 2+m}}.
\]
The rate agrees with the best possible rate known for this problem~\cite{balasubramanian2021nonparametric}. However, the estimate proposed in~\cite{balasubramanian2021nonparametric} is based on a combinatoric search with exponentially computational complexity. In contrast, our estimate is polynomial-time solveable. We show that, under the degree monotonicity assumption, our estimate achieves both statical accuracy and computational efficiency. 

Furthermore, the required $\beta$-monotonicity becomes weaker as the tensor order $m$ increases. Recall that a lower value of $\beta$ implies less constrained degree function. We find that the required lower bound threshold $\beta\geq {2m\over (m-1)(m+2)}$ vanishes to zero as $m\to \infty$. 
\end{rmk}

\begin{proof}[Proof of theorem~\ref{thm:main}] 
We decompose the estimation error into three terms, 
\begin{align}
\FnormSize{}{\hat \Theta -\Theta}^2&\leq \KeepStyleUnderBrace{\FnormSize{}{\text{Block}_k(\tY\circ \hat \sigma^{-1}) - \text{Block}_k(\tY\circ \sigma^{-1})}^2}_{\text{Permutation error; Lemmas~\ref{lem:step1}-\ref{lem:step1est}}}+\KeepStyleUnderBrace{\FnormSize{}{\text{Block}_k(\tY\circ \sigma^{-1})-\text{Block}_k(\Theta)}^2}_{\text{Nonparametric error; Lemma~\ref{lem:noise}}}+\KeepStyleUnderBrace{\FnormSize{}{\text{Block}_k(\Theta)-\Theta}^2}_{\text{Approximation error; Lemma~\ref{lem:a}}}\\
&\leq  d^m \text{Loss}^2(\sigma,\hat \sigma) + k^m+{d^m\over k^2}\\
&\leq d^{-\beta(m-1)+m}+ k^m+ {d^m\over k^2}\\
&\leq d^{-\beta(m-1)+m}+d^{m^2\over m+2}
\end{align}
\end{proof}
\begin{lem}[Permutation error]\label{lem:step1}Step 1 in the algorithm yields the permutation error
\[
\text{Loss}(\sigma, \hat \sigma):={1\over d}\max_{i\in[d]}|\sigma(i)-  \hat \sigma(i)|\leq d^{-(m-1)\beta/2},
\]
with probability at least $1-\exp(-d)$. 
\end{lem}

\begin{lem}[Estimation error due to permutation; Lemma 3 in~\cite{chan2014consistent}]\label{lem:step1est}With probability at least $1-\exp(-d)$, 
\[
\FnormSize{}{\text{Block}_k(\tY\circ \hat \sigma^{-1}) - \text{Block}_k(\tY\circ \sigma^{-1})}^2 \leq d^m \text{Loss}^2(\sigma, \hat \sigma).
\]
\end{lem}

\begin{rmk} 
Lemma~\ref{lem:step1est} quantifies the estimation error due to permutation error.
\end{rmk}

\begin{lem}[Denoising error; Lemma 4 in~\cite{chan2014consistent}]\label{lem:noise}With probability at least $1-\exp(-d)$,
\[
\FnormSize{}{\text{Block}_k(\tY\circ \sigma^{-1})-\text{Block}_k(\Theta)}^2 \leq k^m. 
\]
\end{lem}

\begin{lem}[Approximation error from Lee's 0225 note; corrected Lemma 1 in \cite{chan2014consistent}]\label{lem:a}Suppose the true parameter $\Theta$ is from~\eqref{eq:smooth}. For every fixed integer $k\leq d$, we have
\[
\FnormSize{}{\text{Block}_k(\Theta)-\Theta}^2 \leq {d^m\over k^2}.
\]
\end{lem}

\section{Proofs}
\begin{proof}[Proof of Lemma~\ref{lem:step1}] By definition, $\text{deg}(i)$ is the sample average of roughly $d^{(m-1)}$ i.i.d.\ terms except for at most a few diagonal terms. With high probability, the stochastic deviation satisfies
\[
\text{deg}(i)-\hdeg (i) \lesssim {d^{-(m-1)/2}}. 
\]
By definition, 
\begin{equation}\label{eq:ordering}
\text{deg}(1)\leq \text{deg}(2)\leq \cdots \leq  \text{deg}(d).
\end{equation}
The estimated permutation $\hat \sigma$ is obtained based on empirical degree of $\tY$. Since the empirical degree of $\tY$ is $\hdeg \circ \sigma$, we have
\begin{equation}\label{eq:orderingnew}
\hdeg \circ \sigma \circ \hat\sigma^{-1}(1)\leq \hdeg\circ \sigma \circ \hat \sigma^{-1}(2)\leq \cdots \leq \hdeg\circ \sigma\circ \hat \sigma^{-1}(d). 
\end{equation}
Now, for any given index $i$, we examine the error $|i- \hat \sigma\circ \sigma^{-1}(i)|$. By~\eqref{eq:ordering} and~\eqref{eq:orderingnew}, we have
\[
i=|\KeepStyleUnderBrace{\{j\colon \text{deg}(j)\leq \text{deg}(i)}_{=:\text{I}}\}|, \quad \text{and}\quad \hat \sigma\circ \sigma^{-1}(i)=|\KeepStyleUnderBrace{\{j\colon \hdeg (j)\leq \hdeg  (i)\}}_{=:\text{II}}|, 
\]
where $|\cdot|$ denotes the cardinality of the set. We claim that the sets I and II differ only in at most $d^{(m-1)\beta/2}$ elements. To prove this, we partition the nodes in $[d]$ in two cases.
\begin{enumerate}
\item long-distance nodes in $\{j\colon |i- j|\gg d^{1-(m-1)\beta/2}\}$. In this case, the ordering of $(i,j)$ remains the same in~\eqref{eq:orderingnew} and~\eqref{eq:ordering}, i.e, 
\begin{equation}\label{order}
\text{deg}(i)< \text{deg}(j) \quad \Longleftrightarrow \quad \hdeg(i)<\hdeg(j).
\end{equation}
The $\Longrightarrow$ in~\eqref{order} is because
\[
&\hdeg(j)-\hdeg(i)\geq   \KeepStyleUnderBrace{\left\{\hdeg(j)-\text{deg}(j)\right\}}_{\leq d^{-(m-1)/2}}-\KeepStyleUnderBrace{\left\{\hdeg(i)-\text{deg}(i) \right\}}_{\leq d^{-(m-1)/2}}+\KeepStyleUnderBrace{\{\text{deg}(j)-\text{deg}(i)\}}_{\gg d^{-(m-1)/2}}> 0,
\]
where the third term in the inequality is due to $\beta$-smoothness of deg($\cdot$) and the assumption $|j-i|\gg d^{1-\beta(m-1)/2}$. The other direction in~\eqref{order} can be similarly proved. 
Therefore, we conclude that none of long-distance nodes belong to I $\Delta$ II. 
\item short-distance nodes in $\{j\colon |j-i|\leq d^{1-\beta(m-1)/2}\}$. In this case, \eqref{eq:orderingnew} and~\eqref{eq:ordering} may yield different ordering of $(i,j)$. 
\end{enumerate}
Combining the above two cases gives that
\[
\{j\colon |j-i|\leq d^{1-\beta(m-1)}\} \supset \text{I}\Delta \text{II}. 
\]
Therefore, 
\[
\text{Loss}(\sigma, \hat\sigma):={1\over d}\max_{i}|\sigma(i)-  \hat \sigma(i)|\leq {1\over d}|\text{I}\Delta \text{II}|\leq d^{-\beta(m-1)/2}. 
\]
\end{proof}

\section{Further thoughts}
Step 1 is equivalent to 
\begin{equation}\label{eq:form}
\hat \tau = \argmin_{\tau \colon[d]\to[d]}\sum_{i\in[d-1]}\text{dist}(\tau (i),\tau(i+1)), \quad \text{where}\quad \text{dist}(x,y):=|\hdeg \circ \sigma (x)-\hdeg\circ \sigma (y)|.
\end{equation}
The optimization~\eqref{eq:form} has closed form solution under the degree-based distance function. Specifically, the optimizer of~\eqref{eq:form} is uniquely determined by the sorting
\[
\hdeg \circ \sigma \circ \hat \tau(1)\leq \cdots \leq \hdeg\circ  \sigma\circ\hat \tau(d). 
\]
Can the above framework incorporate the neighborhood estimator in~\cite{zhang2015estimating,airoldi2013stochastic}? List the corresponding Lemmas~\ref{lem:step1}-\ref{lem:a} for the estimator in~\cite{zhang2015estimating}. Which steps make the estimate~\cite{zhang2015estimating} less optimal?

\bibliography{tensor_wang}
\bibliographystyle{plain}

\end{document}

\newpage
\begin{proof}
We first consider the bound for a fixed permutation $\tau\colon[d]\to[d]$. Based on the definition of $\text{Block}_k(\cdot)$, 
 \begin{align}
\FnormSize{}{\text{Block}_k(\tY\circ \tau^{-1}) - \text{Block}_k(\Theta\circ \sigma\circ \tau^{-1})}^2=\FnormSize{}{\text{Block}_k(\tE\circ \tau^{-1})}^2 \stackrel{\tD}{\sim}\FnormSize{}{\text{Block}_k(\tE)}^2. \\
\end{align}
The right hand side $\FnormSize{}{\text{Block}_k(\tE)}^2$ is a sum of i.i.d.\ $k^m$ terms, with weight $(d/k)^m$ on each term. Direct application of large deviation bound shows that 
\[
\FnormSize{}{\text{Block}_k(\tE)}^2=\sum_{\text{$k^m$ blocks in total}} \left(d\over k\right)^m\text{Average}^2\{ \tE(\omega)\colon \text{$d^m/k^m$ many distinct $\omega$'s}\} \leq t^2,
\]
with probability at least $1-k^m\exp(-t^2)$. The uniform bound over all possible permutations $\tau\colon[d]\to[d]$ gives
\[
 \sup_{\tau\colon[d]\to[d]}\FnormSize{}{\text{Block}_k(\tY\circ \tau^{-1}) - \text{Block}_k(\Theta\circ \sigma\circ \tau^{-1})}^2 \leq k^m t^2,
\]
with probability at least $1-\exp(-t^2+m\log k+d\log d)$.
The conclusion holds by setting $t\asymp \sqrt{d \log d}$. 
\end{proof}


\begin{lem} 
\end{lem}

My research is in the intersection of statistics, machine learning, and optimization, with a focus on tensor data analysis.
The proposed project is to develop a framework of statistical theory, machine learning methods, and efficient algorithms to analyze high-dimensional multiway array data. Recent advance in high-throughput sequencing technology has fundamentally transformed scientific research into a data-intensive filed. Through developing efficient methods for analyzing ``big data'', I strive to push the boundary of data science research further, in a form that is useful to individuals, academia, industry, and society. 

I have a unique combination of training backgrounds in mathematics (2006-2010), statistics (2010-2015), genetics (2015-2018), and computer science (2015-2018). I use mathematical language in my work, and math steers my creation to understand the fundamentals of the problems. On the other hand, I feel mostly fulfilled when the developed tools help scientists to make discoveries in the domain field. During my career, I am fortunately to have worked closely with many fellow scientists on cutting-edge big data problems, which have spurred new goals in my research and shaped my scientific approach. {\bf The prevailing theme in my research is to develop machine learning theory and methods to address multi-task, higher-order data challenges.} In this regard, my work will link machine learning and biomedical science, the two areas I have been working on extensively, and also develop new machine learning areas based on the questions raised in the applied endeavors.


\emph{Big data} has been a popular term one encounters everywhere, and we are told that data will define a new science. Rapid developments in modern technologies have made large-scale data readily available across science and engineering. Tensors, or multi-way arrays, provide a generalized data structure and serve as a foundation in many learning procedures. Methods built on tensors provide powerful tools to capture complex structures that lower-order methods fail to exploit. However, tensor-based methods are fraught with challenges. Extending familiar matrix concepts to tensors is notably difficult, and most computational problems regarding tensors are NP-hard. Analyzing tensor data with increasing dimensionality and growing complexity presents serious challenges --- the classical off-the-shelf methods cannot keep up with them, and new approaches have to be developed. 

Below I give two examples of biomedical tensor data arising from my current collaborations, and I will propose new linkages between tensor data analysis with machine learning research. 

{\bf Multi-tissue, multi-individual gene expression.} A typical multi-tissue experiment collects gene expression profiles from different individuals in a number of tissues. The recent completion of Genotype-Tissue Expression (GTEx, Figure 1a) project has provided unprecedented opportunities to investigate transcriptome diversity and complexity. The study results in a huge compendium of tensor data consisting of millions of expression measurements from $\sim$ 20,000 genes across 544 individuals and 53 human tissues, including 13 brain regions, adipose, heart, artery, skin, and more. In this setting, variation in the expression levels arises due to contributions specific to genes, tissues, individuals, and interactions thereof. Understanding the multifactorial patterns of whole-genome transcriptome variation is crucial to unravel gene networks and tissue functions, thereby broadly facilitating research efforts to unravel genetic basis for personalized disease.

{\bf Multimodal neuroimaging data analysis.} Neuroimaging data analysis aims to characterize the human brain connectivity in response to stimulus or physiological changes. As of the fall of 2020, the human connectome project (HCP) has released massive datasets representing the anatomical and functional connectivities within human brains from over 1,200 individuals. Adjacency matrices (networks) are common tools to describe the brain connectivity, where edges (connections) join a set of nodes (brain voxels). Different imaging measurements are utilized to construct the brain networks (Figure 1b), including functional magnetic resonance imaging (fMRI), electroencephalography (EEG), and diffusion tensor imaging (DTI). A key feature is that the acquired networks are huge in size and each possesses complex spatial temporal structure. Efficient methods for multi-modal learning are essential for investigating the commonality and variability between the networks. 

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\textwidth]{example.pdf}
\caption{\small (a) GTEx project collects gene expression profiles of over 20,000 genes from 544 individuals across 53 human tissues. Figure modified based on Timpson et al. [2018]. (b) HCP collects multimodal imaging data including EEG, DTI, fMRI from over 1,200 individuals. Figure modified based on Bruno et al. [2011].}
\end{center}
\end{figure} 

In the above two examples and many others applications, scientists are interested in identifying interpretable low-dimensional structure within the high-dimensional tensor data. The research question goes beyond the traditional multivariate analysis: we are interested in the distribution over network-valued ``objects'' where the objects can be images, networks, manifolds, and arrays. {\bf The main approach in my research is to develop a framework of machine learning models, scalable algorithms, and user-friendly software to analyze multi-modal tensor data.} This will allow practitioners to examine complex interactions among tensor entries and between multiple tensors, thereby providing solutions to questions that cannot be addressed by traditional analysis.  

My general strategy is to carve out a broad range of specially-structured tensors that are useful in practice, and to develop efficient machine learning methods for analyzing these high-dimensional tensor data. Accordingly, the proposed projects will provide a machine learning framework in three aspects to facilitate high-dimensional tensor data analysis. The PI’s previous research on bridging tensors to matrices~\cite{wang2017operator,zeng2019multiway,wang2018learning,wang2017tensor,wang2019three,lee2020tensor} has led to a new inquiry on machine learning techniques for tensors. The proposed research is built upon the PI's past breakthrough but also opens up a suite of theory, methodology, and practice for analyzing high-dimensional tensor data in applications. 

{\bf Aim 1: Spectral theory for specially-structured tensors.} Tensors are not simply matrices with more indices; rather, they are objects possessing multilinear algebraic properties. The proposed research will focus on several spectral quantities for deterministic tensors and random tensors. The techniques developed will facilitate the development of novel tensor-based modeling and computations. See Section~\ref{sec:aim1}.

{\bf Aim 2: Unsupervised learning with high-dimensional tensors.} Many real-world multiway datasets are instances of high-dimendional tensors, in which tensor entries take on discrete values (e.g., ranking) or binary indicators 0/1 (e.g., presence vs.\ absence). Discrete measurements are notably harder to analyze because of the limited information. The proposed research will (1) show how current estimators are suboptimal, (2) develop new estimators that obtain faster rates of convergence, and (3) enrich the application domains in signal processing and network analysis. See Section~\ref{sec:aim2}.

{\bf Aim 3: Supervised Tensor Decomposition with Interactive Side Information.} Machine learning prediction problems common arise in daily applications, i.e.,\ identifying segmentations from images, classifying documents into topics, or deciding target customers in recommendation systems. These problems are often formulated as predicting a variable $Y$ from explanatory variables $X$, where the sample available is in the form of pairs $\{(X_i, Y_i): i=1,\ldots,n\}$. In contrast to traditional work that focuses on only univariate response, we will develop a general learning framework with tensor observation as a response, and features on multiple modes as predictors. Incorporating multiple side information greatly improves the prediction accuracy in tensor data. See section~\ref{sec:aim3}.

{\bf Research team and PI's qualification}. The PI is a young female faculty in the Department of Statistics at UW-Madison. The PI is also a core member in the Institute for Foundations of Data Science (IFDS) which is a joint initiative between Computer Science, Statistics, Mathematics and Electronical Engineering. The proposed research is built upon the PI’s established methodological work on functional properties of higher-order tensors~\cite{wang2017operator, wang2017tensor,zeng2019multiway,wang2018learning,lee2020tensor} and applied work on tensor-based genomic research~\cite{wang2019three,wang2018two}. The PI’s previous work has a transformational nature involving connections across diverse disciplines, and the PI strives to push the boundary of interdisciplinary research further.

To accomplish the proposed research goals, the PI plans to devote four months to each of the three projects. Estimation methods, machine learning algorithms, and computational performance will be carefully studied. Applications to data-intensive fields especially in recommendation system and personalized medicine will be emphasized. The funded PhD students and Postdoc will contribute to aims 1--3. Undergraduate students will also participate in various aspects of the research. These aims will be incorporated into research training and machine learning eduction of students. The PI’s group will also develop publicly available software packages that are ready for use by practitioners.


\section{Research Goals and Strategies} 
The proposed project focuses on tensors of order 3 or greater, known as higher-order tensors. A tensor $A \in  \mathbb{R}^{d_1 \times\cdots \times d_K}$ is a higher-order generalization of matrix, and $A$ can be viewed as a multilinear functional that maps $K$-tuples of vectors to numbers: $(x_1,\ldots,x_K)  \mapsto A(x_1,\ldots,x_K) \in \mathbb{R}$. However, higher-order tensors are not simply matrices with more indices; rather, they are data objects possessing special  properties. The computational challenge stems from the gap from linear algebra to multilinear algebra. The multilinearity is an active topic that is currently studied in complexity theory; tensor computation has been shown to closely connect to the long-standing problems in computer science: P vs.\ NP and matrix multiplication. 

Below I outline three main directions I plan to pursue with the Sony research grant. This would build new links between machine learning methodology and scientific discovery, and also produce new areas in which machine learning theory and applications combine and complement each other. 


\section{Computational Foundations for Specially-Structured Tensors} \label{sec:aim1}
This aim focuses on the spectral properties of higher-order tensors. Here I take the operator norm for illustration but other spectral properties can similarly be studied. The operator norm of a tensor is characterized by the best rank-one approximation in the least square sense. Given an order-$K$ tensor, each possible unfolding operation is represented using a partition $\pi$ of $\{1, . . . , K\}$, where a block in $\pi$ corresponds to the set of modes that are combined into a single mode. Figure 2 illustrates an example for $K = 3$. Earlier I have established a new framework representing all possible tensor unfoldings using the partition lattice, and I show that the spectral norm comparison bounds scale polynomially in the tensor dimensions $\{d_n\}$ with powers depending on the corresponding partition and block sizes. The operator norms of all possible tensor unfoldings together define what we coin a ``norm landscape'' on the partition lattice. To our knowledge, this is the first result to provide a full picture of the norm landscape over all possible tensor unfoldings.

\begin{figure}[http]
\centering
\includegraphics[width=.62\textwidth]{flattern.pdf}
\caption{\small An order-$3$ tensor and its matricizations. The set of all possible unfoldings is in one-to-one correspondence with the set of partitions of $\{1,2, 3\}$.
} \label{fig:1}
\end{figure}


My earlier results are already useful in several machine learning applications but should be interpreted with caution: the comparison bounds concerns the worst-case scenarios with arbitrary tensors. Tensors arisen in applications are often specially-structured. The structures of interest include, but are not limited to, low-rankness, sparsity, non-negativity, orthogonality, and block structure. We plan to study a range of spectral properties of these tensors. I expect that our newest attempt that focuses on structured tensors will provide the right framework to take this one step further. We will not only show the spectral properties of general tensors, but also finely disentangle the structured signals with the less-structured stochastic noise. These will provide machine learning foundations to novel tensor-based learning algorithms. 


\mybox[gray!20]{{\bf Breaking previous limits:} We plan to investigate the spectral properties for both deterministic and random tensors. For example, do the higher-order tensors require a completely new modeling paradigm compared to traditional matrix data? Solving these questions would nicely connects multilinear optimization to machine learning applications.}


\section{Beyond low-rankness: nonparametric tensor learning}
Tensor data are rising ubiquitously in modern data science applications, for instance, brain neuroimaging analysis, integrative genomics, and sensor network localization. Trace regression is one of the most commonly used approaches for modeling matrix data \cite{fan2019generalized,hamidi2019low}. The model characterizes the relationship between a scalar response $Y$ and a high dimensional tensor predictor $\mX \in \tX \subset \mathbb{R}^{d_1\times \cdots\times d_K}$ as 
\begin{equation}\label{eq:linear}
Y=\langle \mX, \mB \rangle+ \varepsilon,\ \text{with } \mB \in \mathbb{R}^{d_1\times \cdots\times d_K} \text{ and rank}(\mB)\leq r,
\end{equation}
where $\varepsilon$ is a zero-mean sub-Gaussian noise, and $r\in\mathbb{N}_{+}$ is the tensor rank typically assumed fixed and much smaller than $d$. The function $\mX\mapsto \langle \mX,\mB\rangle=\text{tr}(\mX\mB^T)$ is called the trace effect, where $\text{tr}(\cdot)$ denotes the matrix trace. Over the last decade, the low-rank trace regression \eqref{eq:linear} has been studied intensively in numerous contexts, including tensor regression, tensor completion, multi-task learning, and compressed sensing.
\begin{itemize}
\item{\bf Tensor regression.} Linear trace regression~\eqref{eq:linear} was first proposed to model a matrix-valued predictor, and was later generalized to model an exponential family response with a known link function~\cite{fan2019generalized}. 
\smallskip

\item {\bf Tensor completion.} In addition to the usual regression setting, another application of trace regression \eqref{eq:linear} is matrix completion, where the goal is to impute the missing entries of a partially observed matrix. Suppose the predictor space $\tX$ consists of basis matrices $\ma^T_i\mb_j$ in $\mathbb{R}^{d_1\times d_2}$, with $\ma_i\in\mathbb{R}^{d_1}$ (respectively, $\mb_j\in\mathbb{R}^{d_2}$) being the basis vector with 1 at the $i$-th (respectively, $j$th) position and 0 elsewhere. Let $\mathbb{P}_{\mX}$ be a uniform distribution over $\tX$. Then model \eqref{eq:linear} reduces to a matrix completion problem, $Y_{ij}=\langle \ma^T_i \mb_j,\mB \rangle +\varepsilon_{ij}= B_{ij}+\varepsilon_{ij}$, where $Y_{ij}, B_{ij}\in\mathbb{R}$ denotes the $(i,j)$-th entry of the data matrix $\mY$ and the signal matrix $\mB$, respectively, for $(i,j) \in \Omega\subset \{1,\ldots,d_1\}\times\{1,\ldots,d_2\}$ in the observed index set. Moreover, the model becomes a matrix denosing problem when the observation set is complete, i.e, $\Omega=\{1,\ldots,d_1\}\times\{1,\ldots,d_2\}$. 
\smallskip

\item {\bf Multi-task learning.} Another application of trace regression is multi-task learning, where the goal is to predict one task response by leveraging the structural similarities among multiple tasks. Here the predictor space $\tX$ consists of only matrices that have a single non-zero row. The multi-task problem collects $n$ observations from $d_1$ different supervised learning tasks. Each task is modeled as a linear regression with an unknown $d_2$-dimensional parameter $\mb_i, i=1,\ldots,d_1$, and the collection of $\mb_i$ forms the rows of $\mB$. The model exploits similarities among multiple tasks to predict the response of the $i$-th task . 
\smallskip

\item {\bf Compressed sensing.} Compressed sensing is also a special application of trace regression, where the goal is to recover the structured matrix $\mB$ from multiple linear combinations of the entry observations. The space $\tX$ is the family of measurement matrices given the sampling schemes. For example, Gaussian ensembles use random matrices $\mX$ with i.i.d.\ entries from a standard normal distribution , while factorized ensembles use rank-1 matrices $\mX=\bmu\mv^T$ for two random vectors $\bmu\in\mathbb{R}^{d_1}, \mv\in\mathbb{R}^{d_2}$ \cite{recht2010guaranteed}.
\end{itemize}


\section{Stochastic tensor block model}
Despite many celebrated results in tensor data analysis under the low-rank formulation, such as tensor regression, tensor completion, tensor PCA, and generalized tensor learning, another important model, \emph{multiway tensor block model} (see Section \ref{sec:TBM} for its definition), has not been well studied yet. Figure~\ref{fig:model-illu} shows an order-3 tensor with a block structure, in which each of the modes is partitioned into several clusters. The goal is to identify the block structure (clustering), as well as to recover the whole tensor data (estimation), from a noisy observation. In comparison with the low-rankness \eqref{eq:Tucker-low-rank}, the discrete block structure is more interpretable because the loading matrix $\mM_k$ encodes the clustering membership along the $k$th mode of the tensor. The tensor block model and high-order clustering arise commonly in practical applications. For example,
{\bf Multi-tissue Gene Expression Analysis.} Gene expression profiles such as scRNA-seq and microarrays are collected from multiple individuals across numbers of tissues. Genes involved in the same biological function typically exhibit similar expressions for some group of tissues and individuals, while these expression values vary from group to group. Similarly, tissues/individuals exhibit clustering patterns due to the similarity therein. It is scientifically interesting to investigate the complex interactions among these three entities. 



\subsection{Unsupervised Learning with High-dimensional Tensor Data}\label{sec:aim2}
My goal in this direction is to develop unsupervised learning tools, i.e., clustering, denoising, and dimension reduction, for high-dimensional tensors, where the learner has little or no label information during training. This is an area where modeling and validation are notably difficult due to missing information. My group has recently developed an efficient tensor decomposition method to successfully estimate salient blocks in the multi-relational network data. Our clustering algorithm discovers community structure with higher accuracy, while being $11\times$ faster, than the competing methods~\cite{wang2019three,zeng2019multiway,wang2018learning}. We are currently generalizing the methods for integrative analysis of multimodal networks. The work will unlock several ambitious directions that lead to robust decision-making in science and engineering applications including electronic industry.  


\subsubsection{Interplay between Statistical Performance and Computational Efficiency}
Tensor decomposition problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. Developing efficient and accurate algorithms for tensor decomposition receives much attention recently. My goal is to develop efficient tensor methods with theoretical guarantees and to make use of the powerful tensor tools for elucidating complex structure in multi-mode data. In the simplest form, tensor decomposition can be formulated as finding the latent factors $\{\mathbf{u}_r\}$ from a noisy tensor $\hat \tA\in(\mathbb{R}^{d})^{\otimes K}$,
\begin{figure}[H]
\centering
\includegraphics[width=.5\textwidth]{nearly_SOD2.pdf}
\caption{Schematic figure for unsupervised tensor decomposition}\label{fig:3}
\vspace{-.5cm}
\end{figure}
where the $\Theta:=\sum_{r=1}^R \lambda_r \mathbf{u}_r^{\otimes K}$ is called the signal tensor, $\{\mathbf{u}_r\}$ is a set of unit vectors in $\mathbb{R}^d$, $\lambda_i$s are positive scalars in $\mathbb{R}$, and the perturbation $\tE$ represents a small, stochastic noise tensor. 

Classical machine learning theory only studies the asymptotical estimation properties when the computational resource is unlimited. However, modern data science is more concerned on the trade-off between statistical limit and computational efficiency. In this regards, we aim to study the fundamental problems of structured tensor decomposition, including the signal-to-noise ratio under which a reliable algorithm is possible, the intrinsic hardness, in terms of minimax rate, of the estimation problem, the estimation properties that are specific to a particular algorithm, and those general to all algorithms. My preliminary results reveal that, for a broad range of low-rank tensors, the empirical estimate often exhibits a two-component error bound:
\[
\text{Loss}(\hat \Theta^{(t)}, \Theta_{\text{true}}) \leq \KeepStyleUnderBrace{C_1 \rho^t\text{Loss}(\hat \Theta^{(0)},\ \Theta_{\text{true}})}_{\text{algorithmic error}}+\KeepStyleUnderBrace{C_2 d^{-(K-1)/2}}_{\text{statistical error}},
\]
where $\hat \Theta^{(0)}$ is the initialization, $\hat \Theta^{(t)}$ is the estimation at the $t$-th iteration, $\rho\in(0,1)$ is a contradiction parameter, and $C_1,C_2>0$ are two constants that do not depend on dimensions. This bound reveals the interesting interplay between the computational and statistical errors, and the result immediate suggests a practical tradeoff between the two aspects. Understanding the gap between the algorithm property and statistical optimality is one of our main goals, which will in turn offer a useful guide to algorithm designs.


\subsubsection{Preliminary Applications to Human Connectome Project (HCP)} 
My group is currently applying the tensor decomposition methods to neuroimaging studies~\cite{geddes2016human}. We use the unsupervised tensor method to analyze an ordinal tensor consisting of structural connectivities among 68 brain regions for 136 individuals from Human Connectome Project (HCP). Each entry in the HCP dataset takes value on a nominal scale, \{{\it high, moderate, low}\}, indicating the strength level of fiber connection. We convert the dataset to a 3-level ordinal tensor $\tA\in[3]^{68\times 68\times 136}$, and apply the ordinal tensor method with a logistic link function. Based on the estimated tensor factors $\mathbf{\hat u}_r$, we perform a clustering analysis via K-mean on the brain nodes. The 68 brain nodes are grouped into 11 clusters. We find that the clustering captures the spatial separation between brain regions very well (Table~\ref{table:clustering}). In particular, cluster I represents the connection between the left and right hemispheres, whereas clusters II-III represent the connection within each of the half brains (Figure~\ref{figure:brain image}). Other smaller clusters represent local regions driving by similar nodes. For example, the cluster IV/VII consists of nodes in the supramarginal gyrus region in the left/right hemisphere. This region is known to be involved in visual word recognition and reading. The identified similarities among nodes without external annotations illustrate the potential power of tensor methods to clustering analysis. 


\begin{figure}[ht]
\centering
\includegraphics[width = .7\textwidth]{HCP.pdf}
\caption{Tensor clustering methods applied to neuroimaging data. Cluster I reflects the connections between two brain hemispheres (a). Cluster II/III consists of nodes within left/right hemisphere (b-c). Node names are shown in abbreviation. Edges are colored based on predicted connection level averaged across individuals. }  \label{figure:brain image}
\end{figure}


\begin{table}[ht]
\center
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
Cluster     & \multicolumn{3}{c|}{I}\\ \hline
Brain nodes & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}l@{}} l.frontalpole, l.temporalpole, l.medialorbitofrontal,  l.cuneus, l.parahippocampal, l.lingual,            \\
r.frontalpole, r.temporalpole,  r.medialorbitofrontal,  r.cuneus,  r.parahippocampal \\
\end{tabular}}
\hline
Cluster & \multicolumn{3}{c|}{II}\\ \hline
Brain nodes & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}l@{}}
l.caudalmiddlefrontal,  l.inferiorparietal,  l.insula,  l.isthmuscingulate, l.lateraloccipital(2), \\
l.parsopercularis, l.parstriangularis, l.postcentral, l.precuneus, l.superiorfrontal, l.superiortemporal(3)
\end{tabular}} \\ 
\hline
Cluster & \multicolumn{3}{c|}{III}\\ \hline
Brain nodes & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}l@{}}
r.caudalmiddlefrontal, r.inferiorparietal, r.insula, r.isthmuscingulate, r.lateraloccipital(2), r.lingual, \\
r.parsopercularis, r.parstriangularis, r.postcentral, r.precentral, r.precuneus, r.superiorfrontal(3), \\
r.superiorparietal,  r.superiortemporal(3)
\end{tabular}} \\ 
\hline
Cluster      & IV    & V  & VI\\
\hline
Brain nodes & \begin{tabular}[c]{@{}c@{}}l.supramarginal(4)\end{tabular}&
\begin{tabular}[c]{@{}c@{}}l.inferiortemporal(3)\end{tabular}&
\begin{tabular}[c]{@{}c@{}}l.middletemporal(3)\end{tabular}\\
\hline
Cluster & VII  & VIII & VIIII\\
\hline
Brain nodes &
\begin{tabular}[c]{@{}c@{}}r.supramarginal(4)\end{tabular}&
\begin{tabular}[c]{@{}c@{}}r.inferiortemporal(3)\end{tabular}&
\begin{tabular}[c]{@{}c@{}}r.middletemporal(3)\end{tabular}\\
\hline
Cluster &X & \multicolumn{2}{c|}{XI}  \\
\hline
Brain nodes &
\begin{tabular}[c]{@{}c@{}}l.superiorfrontal(2)\end{tabular}&
 \multicolumn{2}{c|}{l.precentral,  l.superiorparietal}\\
 \hline
\end{tabular}
}
\end{sc}
\caption{Node clusters in the HCP analysis. The first alphabet in the node name indicates the left (L) or right (R) hemisphere. The number in the parentheses indicates the node count in each cluster. }  \label{table:clustering}
\end{table}

\mybox[gray!20]{
{\bf Impact and Significance:} 
\begin{itemize}[leftmargin=*]
\item Develop interactive data exploration and visualization for unsupervised learning tasks such as tensor clustering, density estimation, and dimension reduction. 
\item Design robust tools to analyze tensor data and extract hidden information to aid decision making. 
\item Address data mining challenges from information gathering to pattern recognition. 
\end{itemize}
 }
\subsection{Supervised Tensor Decomposition with Interactive Side Information}\label{sec:aim3}

Multi-dimensional arrays are often collected with side information on multiple modes in modern scientific and engineering studies. A popular example is in neuroimaging~\cite{geddes2016human}. The brain connectivity networks are collected from a sample of individuals, accompanied by individual characteristics such as age, gender, and diseases status (see Figure~\ref{fig:intro1}a). Another example is in network analysis~\cite{pmlr-v108-berthet20a,hoff2005bilinear}. A typical social network consists of nodes that represent people and edges that represent the friendships. Side information such as people’s demographic information and friendship types are often available. In both examples, it is of keen scientific interest to identify the variation in the tensor data (e.g., brain connectivities, social community patterns) that is affected by available features. These seemingly different scenarios pose a common yet challenging problem for tensor data modeling. 

In addition to the aforementioned challenges, many tensor datasets consist of non-Gaussian measurements. Examples include the political interaction dataset \cite{hu2015scalable} which measures action counts between countries under various relationships, and the brain connectivity network dataset \cite{zhang2018mapping} which is a collection of binary adjacency matrices. Classical tensor decomposition methods are based on minimizing the Frobenious norm of the reconstruction error, leading to suboptimal predictions for binary- or count-valued response variables. A number of supervised tensor methods have been proposed \cite{narita2012tensor} to address the tensor regression problem in various forms (e.g.\ scalar-to-tensor regression, tensor-response regression). These methods often assume Gaussian distribution for the tensor entries, or impose random designs for the feature matrices, both of which are less suitable for applications of our interest. The gap between theory and practice means a great opportunity to improve modeling paradigms and better capture the complexity in tensor data. 

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=16cm]{demo.pdf}
\end{center}
\caption{Examples of supervised tensor decomposition with interactive side information. (a) Network population model. (b) Spatial-temporal growth model as an example of supervised decomposition of an order-3 tensor $\tY$, where feature matrices $\mX_k$ are available on each of the three modes.}\label{fig:intro1}
\end{figure*}

\subsubsection{Proposed Framework for Supervised Tensor Learning} 
The proposed project focuses on developing a general model for decomposing a data tensor with exponential family entries and interactive side information. We will formulate the learning task as a structured regression problem, with tensor observation serving as the response, and the multiple side information as interactive features. Figure~\ref{fig:intro1}b illustrates our model in the special case of order-3 tensors. A low-rank structure is imposed to the conditional mean tensor, where unlike classical decomposition, the tensor factors $\mX_k\mM_k\in\mathbb{R}^{d_k\times r_k}$ belong to the space spanned by features $\mX_k\in\mathbb{R}^{d_k\times p_k}$ for $k=1,2,3$. The unknown matrices $\mM_k\in\mathbb{R}^{p_k\times r_k}$ (referred to as ``dimension reduction matrices'') link the conditional mean to the feature spaces, thereby allowing the identification of variations in the tensor data attributable to the side information. The tensor approach boosts the prediction performance by incorporating interactions across multiple modes. 


Our proposal blends the modeling power of generalized linear model (GLM) and the exploratory capability of tensor dimension reduction in order to take the best out of both worlds. We propose to leverage GLM to allow heteroscedacity in the non-Gaussian data. This flexibility is important in practice. Furthermore, our low-rank model on the possibly non-linearly transformed conditional mean tensor effectively mitigates the curse of high dimensionality. In classical GLM, the sample size and feature dimension are well defined; however, in the tensor data analysis, we observe only one realization of an order-$K$ tensor and up to $K$ interactive feature matrices. Both the number of tensor entries and feature dimension grow exponentially in $K$. Dimension reduction is therefore crucial for prediction and interpretability. We will establish the computational performance of our estimator, and we will quantify the gain in prediction in applications. 

\subsubsection{Three Examples}
We give three different examples to highlight the applicability of our supervised tensor learning methods.\\

\begin{example}[Spatio-temporal growth model]
The growth curve model~\cite{gabriel1998generalised,srivastava2008models} was originally proposed as an example of bilinear model for matrix data, and we adopt its higher-order extension here. Let $\tY=\entry{y_{ijk}}\in\mathbb{R}^{d \times m\times n}$ denote the pH measurements of $d$ lakes at $m$ levels of depth and for $n$ time points. Suppose the sampled lakes belong to $q$ types, with $p$ lakes in each type. Let $\{\ell_j\}_{j\in[m]}$ denote the sampled depth levels and $\{t_k\}_{k\in[n]}$ the time points. Assume that the expected pH trend in depth is a polynomial of order at most $r$ and that the expected trend in time is a polynomial of order $s$. Then, the conditional mean model for the spatio-temporal growth model can be represented as
\begin{equation}\label{eq:time}
\mathbb{E}(\tY|\mX_1,\mX_2,\mX_3)=\tC\times\{\mX_1\mM_1,\ \mX_2\mM_2,\ \mX_3\mM_3\},
\end{equation}
where $\mX_1=\text{blockdiag}\{\mathbf{1}_p,\ldots,\mathbf{1}_p\}\in \{0,1\}^{d\times q}$ is the design matrix for lake types, and
\[
\mX_2=
\begin{pmatrix}
1 & \ell_1&\cdots &\ell^{r}_1\\
1 & \ell_2&\cdots &\ell^{r}_2\\
\vdots &\vdots&\ddots&\vdots\\
1&\ell_{m}&\cdots&\ell^{r}_{m}
\end{pmatrix},\quad
\mX_3=
\begin{pmatrix}
1 & t_1&\cdots &t^{s}_1\\
1 & t_2&\cdots &t^{s}_2\\
\vdots &\vdots&\ddots&\vdots\\
1&t_{n}&\cdots&t^{s}_{n}
\end{pmatrix}
\]
are the design matrices for spatial and temporal effects, respectively, $\tC\in\mathbb{R}^{r_1\times r_2\times r_3}$ is the unknown core tensor, and $\mM_k$ are unknown dimension reduction matrices along each mode. The factors $\mX_k\mM_k$ are sufficient features for the mean model~\eqref{eq:time}. The spatial-temporal model is a special case of our supervised tensor decomposition model, with features available on each of the three modes.\\
\end{example}


\begin{example}[Network population model]\label{example:brain}
Network response model~\cite{rabusseau2016low, zhang2018network} is recently developed for neuroimanig analysis. The goal is to study the relationship between brain network connectivity pattern and features of individuals. Suppose we have a sample of $n$ observations, $\{(\mY_i, \mx_i)\colon i=1,\ldots,n\}$, where for each individual $i\in[n]$, $\mY_i\in\{0,1\}^{d\times d}$ is the symmetric adjacency matrix whose entries indicate presences/absences of connectivities between $d$ brain nodes, and $\mx_i\in\mathbb{R}^p$ is the individual's feature such as age, gender, cognition score, etc. The network-response model  has the conditional mean
\begin{equation}\label{eq:network}
\textup{logit}(\mathbb{E}(\mY_i|\mx_i))=\tB\times_3\mx_i, \quad \text{for }i=1,\ldots,n,
\end{equation}
where $\tB\in \mathbb{R}^{d\times d\times p}$ is a rank-$(r_1,r_1,r_2)$ coefficient tensor that is symmetric in the first two modes.  

The model~\eqref{eq:network} is a special case of our supervised tensor decomposition, with feature matrix on the last mode of the tensor. Specifically, we stack the network observations $\{\mY_i\}$ together and obtain an order-3 response tensor $\tY\in\{0,1\}^{d\times d\times n}$. Define a feature matrix $\mX=[\mx_1,\ldots,\mx_n]^T\in\mathbb{R}^{n\times p}$. Then, the model~\eqref{eq:network} has the equivalent representation of supervised tensor decomposition,
\[
\textup{logit}(\mathbb{E}(\tY|\mX))=\tC\times\{\mM,\ \mM,\ \mX\mM'\},
\]
where $\tC\in\mathbb{R}^{r_1\times r_1\times r_2}$ is the core tensor, $\mM\in\mathbb{R}^{d\times r}$ is the dimension reduction matrix at the first two modes, and $\mM'\in\mathbb{R}^{p\times r_2}$ is for the last mode.\\
 \end{example}
 
 \begin{example}[Dyadic data with node attributes] Dyadic dataset consists of measurements on pairs of objects or under a pair of conditions. Common examples include graphs and networks. Let $\tG=(V,E)$ denote a graph, where $V=[d]$ is the node set of the graph, and $E\subset V\times V$ is the edge set. Suppose that we also observe feature vector $\mx_i\in\mathbb{R}^p$ associated to each $i\in V$. A probabilistic model on the graph $\tG=(V,E)$ can be described by the following matrix regression. The edge connects the two vertices $i$ and $j$ independently of other pairs, and the probability of connection is modeled as
\begin{equation}\label{eq:edge}
 \textup{logit}\left(\mathbb{P}\left((i,j)\in E\right)\right)=\mx^T_i\mB\mx_j=\langle \mB,\ \mx^T_i\mx_j\rangle,
 \end{equation}
 where $\mB\in\mathbb{R}^{p\times p}$ is a symmetric rank-$r$ matrix. The low-rankness in $\mB$ has demonstrated its success in modeling transitivity, balance, and communities in the networks~\cite{hoff2005bilinear}. Our supervised tensor decompostion also incorporates the graph model as a special case. Let $\tY=\entry{y_{ij}}$ be a binary matrix where $y_{ij}=\mathds{1}_{(i,j)\in E}$. Define $\mX=[\mx_1,\ldots,\mx_n]^T\in\mathbb{R}^{n\times p}$. Then, the graph model~\eqref{eq:edge} can be expressed as
 \[
 \textup{logit}(\mathbb{E}(\mY|\mX))=\mC\times\{\mX\mM,\ \mX\mM\}, 
  \]
  where $\mC\in\mathbb{R}^{r\times r}$ and $\mM\in\mathbb{R}^{p\times r}$ are based on the singular value decomposition of $\mB=\mM\mC\mM^T$. 
  \end{example}
  

\subsubsection{Significance and Challenges}
In the above three examples and many other studies, researchers are interested in uncovering the variation in the data tensor that can be explained by features. The coefficient tensor $\tB$ in model~\eqref{eq:network} serves this goal by collecting the feature effects. To encourage the sharing among effects, decomposition in Figure~\ref{fig:intro1}b assumes that the feature effects $\tB$ lie in a low-dimensional parameter space. The low-rank assumption is plausible in many scientific applications. In the Example~\ref{example:brain} of brain imaging analysis, for instance, it is often believed that the brain nodes can be partitioned into several regions, and the person's attributes affect the connectivity in a similar way within a region. 

Learning tensors with interactive side information is challenging because of the extremely high dimensionality in both the tensor space and feature space. The PI's previous extensive experience on tensor related work has shown that tensors sought in applications often possess special structures, such as (nearly) low-rankness, sparsity, non-negativity, or orthogonal decomposability. We will leverage the formalisms of \emph{intrinsic dimension} to develop efficient statistical methods for analyzing these high-dimensional datasets. We will further develop adaptive, semi-supervised methods that incorporate practical constraints in industrial applications, such as incomplete observation with missing labels, corrupted distributions, and computation with time and memory constraints. 

\mybox[gray!20]{{\bf Impact and Significance:} 
\begin{itemize}[leftmargin=*]
\item Develop efficient prediction tools for tensor classification, tensor regression, and deep tensor neural network in the presence of domain constraints.
\item Build data-driven prototypes that integrate machine learning into the data-to-decision process. 
\item Improve efficiency in scientific discovery using powerful multi-modal brain imaging analyses. 
\end{itemize}
}


\section{Borader Impacts}
The educational components of this proposal are targeted at several different populations of students: PhD students, undergraduate students, visiting international students, AP statistics students underrepresented students at Mid-west area of United States. The project will support the development of the PI, a young faculty, to create an interdisciplinary working group of researchers.


\subsection{Diversity and My Past Experience}
I am one of the few female faculty members in my home department. As a young faculty, I have been striving to encourage more under-represented students into the field of machine learning. I am currently starting my third year in the tenure track, and I have successfully mentored a number of female undergraduate students -- two of which are embarking their PhD studies at top US universities this fall. Female students are traditionally underrepresented in my filed, and I have been striving to retain more women in the academic research. For that reason, I have sponsored my students to present their research in top-notch machine learning conferences including Conference on Neural Information Processing Systems (NeurIPS) and International Conference of Machine Learning (ICML). My experience of recruiting under-represented students has attracted social media coverage, and I was recently featured in the article ``Women in STEM: 5 Thoughtful Ways to Recruit and Retain Them'' by Course Hero Faculty Club . With the support of Sony research grant, I will be excited to further extend this effort to a larger student body. 


In addition, I have been devoted myself to general higher education. I am one of the core members in Madison Teaching and Learning Excellence program. I plan to create an undergraduate course on introduction to data sciences. This course will expose undergraduate students to a wide range of modern exploratory analytic tools for massive high-dimensional data, and will be closely tied with data-intensive scientific domains. Machine learning, as a discipline, is concerned with transforming “data” into “information”. However data are not just numbers, they are numbers with social, economical, and cultural contexts. The personnel constitution in our education environment is becoming increasingly diverse, reflecting the variety witnessed in our broader society. Diversity can be measured across many dimensions – gender, ethnicity, origin, age, etc. However, diversity is not limited to these visible physical differences; it also consists of difference in each student’s personality (introvert vs. extravert), learning skills (listening, speaking, reading, or writing skills), emotional development, and every aspect that distinguishes one individual from another. I believe that every student is unique in their own way, and I have been striving to create an open, inclusive education environment that prepares students to become better professionals in the larger, diverse society.

\subsection{Deliverables and Milestones}
The project will create deliverables in three aspects: {\bf research}, {\bf education}, and {\bf social good}. I plan to devote one year for each of the aforementioned research problems. My current group consists of three PhD students and two undergraduates, of which three are females (me included) and three are males. I have actively contributed to several multi-institutional consortiums, in collaboration with researchers in United States, Europe, and Asia. The Sony grant will support me to further create a diverse working group. Regarding teaching, I will create a new course on \emph{data science} in both traditional and online formats. The new course will introduce students to the real word challenges in implementing statistical machine learning approaches to decision making. I will also organize a series of workshops on the scientific applications of tensor data analysis, with the aim to encourage interdisciplinary collaborations. Figure~\ref{fig:proposal} summarizes the planned milestones for my proposal. 


\begin{figure}[!h]
\begin{center}
\includegraphics[width=.98\textwidth]{milestone.pdf}
\vspace{-.4cm}
\caption{Planned deliverables and milestones for my proposal.}\label{fig:proposal}
\end{center}
\vspace{-.4cm}
\end{figure}

\subsection{Other Outreaches}
I have a unique combination of training backgrounds in statistics, computer science, and scientific research. My established collaborations on cutting-edge scientific problems has motivated the methodological ideas presented in this proposal. Ongoing collaborations include research groups at University of Pennsylvania, University of Chicago, University of California-Berkeley, and data science group at Chan Zuckerberg Biohub. The resulting solutions are expected to have a direct impact on a broad range of scientific and engineering fields. Open-source software will be released, as the fruit of the research, that facilitates academia, industry, and society to analyze complicated large-scale data. 

\bibliography{tensor_wang}
\bibliographystyle{plain}


\end{document}