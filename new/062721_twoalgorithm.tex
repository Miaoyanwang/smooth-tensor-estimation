\documentclass[10pt]{article}
\usepackage{setspace}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{fancybox}
\usepackage{algorithm, algpseudocode}
\usepackage{url}

\usepackage{multirow}
\usepackage{color}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{comment}
\usepackage{bm}
\usepackage{enumitem}
\usepackage[breakable, theorems, skins]{tcolorbox}
\DeclareRobustCommand{\mybox}[2][gray!20]{%
\begin{tcolorbox}[   %% Adjust the following parameters at will.
        breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\dimexpr\textwidth\relax, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=0pt,outer arc=0pt,
        ]
        #2
\end{tcolorbox}
}


\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}


\usepackage[margin=1in]{geometry}
 
\allowdisplaybreaks[4]
\usepackage{bbm}


\usepackage{amsrefs}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}


\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor = blue,
    linkcolor=blue,
    filecolor=magenta,           
    urlcolor=black,
}

\usepackage[breakable, theorems, skins]{tcolorbox}
\DeclareRobustCommand{\mybox}[2][gray!20]{%
\begin{tcolorbox}[   %% Adjust the following parameters at will.
        breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\dimexpr\textwidth\relax, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=0pt,outer arc=0pt,
        ]
        #2
\end{tcolorbox}
}

\def\sign{\textup{sgn}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{model}{Model}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}[section]

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\usepackage{dsfont}

\newcommand{\hdeg}{\widehat{\textup{deg}}}


\theoremstyle{definition}
\newtheorem{open}[]{Open problems}

 \usepackage[parfill]{parskip}

\usepackage[compact]{titlesec}

\usepackage{sectsty}
\sectionfont{\fontsize{11}{10}\selectfont}
\subsectionfont{\fontsize{10.5}{10}\selectfont}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*0}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}

\input macros.tex

\begin{document}
\begin{center}
%{\bf \large High-dimensional Tensor Learning: The Good, the Bad, and the Pragmatic}\\
{\bf \large Polynomial-time estimation of permutation equivarant tensors}\\

{Miaoyan Wang, June 27, 2021}
\end{center}
Consider an order-$m$ permutation equivariant tensor
\begin{equation}\label{eq:smooth}
\tY=\Theta\circ \sigma +\tE,
\end{equation}
where $\tE\in\mathbb{R}^{d\times\cdots \times d}$ is a symmetric mean-zero sub-Gaussian noise tensor, $\sigma\colon[d]\to[d]$ is an unknown permutation, and $\Theta$ is an unknown signal tensor sampled from a Lipschitz symmetric function with fixed design
\[
 \Theta(i_1,\ldots,i_m)=f\left({i_1\over d},\ldots,{i_m\over d}\right), \quad \text{for all }(i_1,\ldots,i_m). 
 \]
\section{Subclass I: monotonic degree}

\begin{assumption}[$\beta$-monotonic degree]\label{ass:degree}
We call a smooth tensor $\Theta\in\tP(L)$ is degree-identifiable, if there exists a constant $\beta\in[0,1]$ and a small tolerance $\varepsilon_d\lesssim d^{-(m-1)/2}$ such that
\begin{equation}\label{eq:mono}
\text{deg}(i)-\text{deg}(j)\gtrsim \left({i-j\over d}\right)^{1/\beta}-\varepsilon_d, \quad \text{for all }i\geq j \in[d].
\end{equation}
\end{assumption}
\begin{rmk} 
The condition~\eqref{eq:mono} assumes the polynomial growth of population degree function up to a small error. The tolerance $\tO(d^{-(m-1)/2})$ allows for small fluctuations within statistical accuracy. We call $\beta$ the signal level, because it quantifies the identifiability of permutation from the degree. A lower value  of $\beta$ implies flatness of the function. We make the convention that a constant degree function is $0$-monotonic. 
\end{rmk}
\begin{thm}[Sorting-and-blocking under $\beta$-monotonicity]\label{thm:main}
Consider model~\ref{eq:smooth} under Assumption~\ref{ass:degree}. Consider the sorting-and-blocking algorithm with number of blocks $k=d^{m\over 2+m}$. When $\beta \geq {2m\over (m-1)(m+2)}$, the algorithm output attains the optimal estimation rate 
\[
\tR(\hat \Theta_{\text{LS}}, \Theta) \leq d^{-{2m\over 2+m}}.
\]
\end{thm}

\section{Subclass II: non-constant fluctuation}
\begin{assumption}[$\beta$-detectable functions]\label{ass:detectable} A function $f$ is called weakly $\beta$-detectable, if the function $f$ has at least a local $(1/\beta)$-polynomial fluctuation in each coordinate,
\begin{equation}\label{eq:detectable}
\max_{y\in[0,1]} |f\left(y,\ \mx_{-1}\right) - f\left(y+d^{-1}, \mx_{-1}\right)|\geq d^{-1/\beta} \quad \text{for all }\mx_{-1}\in[0,1]^{m-1},
\end{equation}
where we use the shorthand $\mx_{-1}= (x_2,\ldots,x_m)$ to denote the $(m-1)$ coordinates except the first one.  
\end{assumption}

\begin{rmk} 
The exponent $\beta$ quantifies the signal level of the function. A lower value of $\beta$ implies global flatness of the function (low signal), whereas a high value of $\beta$ implies polynomial fluctuation (high signal). By convention, a constant function has $\beta=0$. We view the condition~\eqref{eq:detectable} as a mild non-degeneracy condition because it precludes nearly constant function in certain coordinates. In the latter case one may reduce the $m$-order tensor to the problem of $(m-1)$-order tensor.  
\end{rmk}

\begin{thm}[Iterative blocking under $\beta$-detectability]Consider model~\ref{eq:smooth} under Assumption~\ref{ass:detectable}. Consider the iterative tensor block algorithm~\cite{han2020exact} with number of blocks $k=d^{m\over 2+m}$. When $\beta \geq 4/m$, the algorithm output attains the optimal estimation rate 
\[
\tR(\hat \Theta_{\text{LS}}, \Theta)\leq d^{-{2m\over 2+m}}.
\]
\end{thm}


\begin{cor}[Blessing of orders for bi-lipschitz tensors] Consider the smooth tensor model~\eqref{eq:smooth} with order $m\geq 4$. Furthermore, suppose $f$ is bi-Lipschitz in that
\[
0< l \leq {|f(\omega)-f(\omega')| \over \onenorm{\omega-\omega'}} \leq L < \infty,
\]
for two positive constants $l,L>0$. Then, the algorithm output from~\cite{han2020exact} attains the optimal estimation rate. 
\end{cor}

\section{Summary}
Table~\ref{table:2} shows that the required signal level threshold $\beta$ vanishes to zero as $m\to \infty$. Recall that a lower value of $\beta$ implies less constrained function. Therefore, the required signal condition on $\beta$ becomes weaker as the tensor order $m$ increases. 

\begin{table}[H]
\centering
\begin{tabular}{ccccc}
Model class & MLE (theory) &Algorithm I & Algorithm II & NN smoothing\\
\hline
Assumption & - & local fluctuation & monotonic degree& -\\
Signal level& - & require $\beta \geq {4\over m}$ & require $\beta \geq {2m \over (m-1)(m+2)}$ &-\\
Rate &$d^{-2m/(2+m)}$ & $d^{-2m/(2+m)}$ &$d^{-2m/(2+m)}$& $d^{-\min(2m/(2+m),\ 2(m-1)/3)}^{*}$ \\
\hline
\end{tabular}
\caption{Polynomial algorithms for smooth tensor estimation. $^*$conjecture. $-$ none. }\label{table:2} 
\end{table}

Questions:
\begin{enumerate}
\item We have described two polynomial algorithms and their successful regimes. Which assumptions are more relaxed? 
\item Assumption~\ref{ass:detectable} is derived based on the signal level requirement in~\cite{han2020exact}. See Figure 2 and Equations (11) and (12) in~\cite{han2020exact}. Please verify the sufficiency. Any better reformulation in our context? 
\end{enumerate}

\bibliography{tensor_wang}
\bibliographystyle{plain}

\end{document}