\documentclass{article}

% if you need to pass options to natbib, use, e.g.:

% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
 % \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{neurips_2019}
 \usepackage{enumitem}
% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2019}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{bm}
\usepackage{subfig}
\usepackage[english]{babel}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{appendix}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{ass}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{exam}{Example}
\newtheorem{proof}{Proof}
\input macros.tex
\usepackage{dsfont}

\usepackage{multirow}
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\DeclareMathOperator*{\minimize}{minimize}



\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice 
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}



\title{Nonparametric Tensor Estimation and Completion via Hypergraphon Learning}


\author{%
Miaoyan Wang \\
 University of Wisconsin -- Madison\\
\texttt{miaoyan.wang@wisc.edu}
}

\begin{document}

\maketitle


\begin{abstract}
We consider the problem of tensor estimation from noisy observations with possibly missing entries. A nonparametric approach to tensor estimation is developed based on $K$-uniform hypergraphons. The hypergraphon model captures the key features of conditional independence arising in the generative process of multiway tensor data. The nonparametric hypergraphon representation of tensors encompasses many existing tensor models---such as CP models, Tucker models, shape constrained tensor models---as special examples. We develop a rate-optimal nonparametric tensor estimator using the stochastic blockmodel approximation to the underlying hypergraphon. A surprising distinction between the hypergraphon-based tensor estimation and high-dimensional nonparametric regression is revealed. Furthermore, we establish the integrated risk bound for the hypergraphon estimation in specially structured functional classes. The result uncovers the joint contribution of the statistical bias-variance error and the agnostic discretization error. Numerical results demonstrate the robustness of our proposal over previous tensor methods and the attractive performance as the tensor order increases.
\end{abstract}

\section{Introduction}
\section{Nonparametric Tensor Model via Hypergraphons}
Let $\tY=\entry{\tY(\mi)}\in\mathbb{R}^{d_1\times \cdots \times d_K}$ be an order-$K$ $(d_1,\ldots,d_K)$-dimensional data tensor. We propose a two-stage generative process for the tensor entries. Firstly, we draw a collection of i.i.d.\ random variables, $x_k(i) \in \tX$ following the probability measure $(\tX,\mu_{\tX})$, for all $i\in[d_k]$, $k\in[K]$. The Cartesian product of the random variables, denoted $(x_1(i_1),\ldots,x_K(i_K))$, represents the latent features at position $\mi=(i_1,\dots,i_K)$ of the tensor. Second, conditional on the latent features, the tensor entries are independently distributed.
\begin{itemize}[label=\textbullet, leftmargin=*]
\item Nonparametric mean: there exists a unknown function $f\colon \tX^K\mapsto [0,1]$ such that
\begin{equation}\label{eq:model}
\mathbb{E}[\tY(\mi)] =f(x_1(i_1),\ldots,x_K(i_K)),\quad \text{for all}\ \mi=(i_1,\ldots,i_K)\in[d_1]\times \cdots \times [d_K].
\end{equation}
Here $x_k(i_k)\in\tX$ denotes the latent feature associated with the $i_k$-th entry along the $k$-th mode of the tensor, for $k\in[K]$. 
\item Conditionally independence: conditional on the latent features $\{x_k(i_k)\}$, the tensor entries $\tY(\mi)$ are independently distributed, mean-zero, sub-Gaussian random variables; i.e. $\mathbb{E}[e^{t\tY(\mi)}]\leq \exp(t\sigma^2)$ for all $\mi\in[d_1]\times \cdots \times[d_K]$ and $t>0$.
\item We set $\tX=[0,1]$ and $\mu_{\tX}$ the Lebesgue measure over $[0,1]$. The latent features $x_k(i_k)$ are mutually independent for all $i_k\in[d_k]$ and $k\in[K]$.
\item Regularity conditions on $f$. Two possible options: 1. stepwise functions, 2. Holder smooth functions.
\end{itemize}

(draw a figure...) We call the model~\eqref{eq:model} the nonparametric tensor model because the mean function $f$ is unknown and thus to be estimated. By specializing the realization of the latent features and the function $f$, model~\eqref{eq:model} incorporates several common previously-studied tensor models as special cases. 

\begin{exam}[CP model] Let $\tX=\mathbb{R}^R$ and $\mx^{(k)}=(x^{(k)}_{1},\ldots,x^{(k)}_{R})^T\in\mathbb{R}^R$ for all $k\in[K]$. Define $f \colon (\mx^{(1)},\ldots,\mx^{(K)})\mapsto \sum_{r} \prod_{k} x^{(k)}_r$. Then model~\eqref{eq:model} can be written as
\[
\mathbb{E}(\tY | \mxi_1,\ldots,\mxi_K)=\sum_r \mxi^{(1)}_r\otimes \cdots\otimes \mxi^{(k)}_r, \ \text{where}\ \mxi^{(k)}_r:=(x^{(k)}_{1},\ldots,x^{(k)}_{d_k})^T \  \text{for }k\in[K].
\]
\end{exam}

\begin{defn}[Uniform random design] A uniform random design over the index set $[d]$ is represented by a stochastic process $\{x_i\}$, where $x_i$ are i.i.d.\ sampled from $\textup{Uniform}[0,1]$ for all $i\in[d]$.
\end{defn}

\begin{defn}[Cartesian product of random designs] An order-$K$ random design over the index set $[d_1]\times \cdots \times [d_K]$ is defined as the Cartesian product of random designs over each marginal index set $[d_k]$ for $k\in[K]$. Specifically, the order-$K$ uniform random design is represented by a collection of random vectors $\{(x^{(1)}_{i_1},\ldots,x^{(K)}_{i_K})\}$, where $x^{(k)}_{i_k}$ are i.i.d.\ sampled from $\textup{Uniform}[0,1]$ for all $i_k\in[d_k]$ and $k\in[K]$.
\end{defn}

\begin{defn}[Hypergraphon] A $K$-uniform hypergraphon is a measureable function $f\colon [0,1]^K\mapsto [0,1]$. 
\end{defn}
The function can be seen as a kernel functions for random tensor models. 
\section*{Acknowledgements}

\bibliographystyle{unsrt}
\bibliography{tensor_wang}



\end{document}
